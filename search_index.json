[
["index.html", "狗熊会人才计划第三期——My Case 前言 致谢", " 狗熊会人才计划第三期——My Case yimeng 2018-09-27 前言 嗨，各位。我写了一本书。这本书是这样的，第 1 章介绍了狗熊会人才计划，第 2 章说了人才计划章程，从第 3 章到第 8 章分别是阶段性TASK，最后第 9 章是Final Project，用xaringan 生成。 其中，TASK中的作品可能是老师给出的优秀作品，也可能是我自己的作品，取决于报告的质量高低。 我用了两个 R 包编译这本书，分别是 knitr (Xie 2015) 和 bookdown (Xie 2018)。 致谢 非常感谢狗熊会各位老师，尤其是水妈和雪姨对我的帮助（催促），使得这些TASK顺利完成。也感谢yalei没有对我的智商放弃，才有了这样一本书。 yimeng 于人民大学 参考文献 "],
["author.html", "作者简介", " 作者简介 yimeng，中国人民大学统计学院本科在读，方向经济统计学，在Data Science的边缘试探。 人生终极的三大梦想： 在日本东京街头吃一碗拉面； 在马来西亚摘一个榴莲； 在北欧的某个小镇看一次极光； 最好实现的一个梦想： 学好统计，写好代码，先挣钱再说。 "],
["intro.html", "第 1 章 狗熊会人才计划介绍 1.1 狗熊会，what’s that?! 1.2 人才计划，OMD! 1.3 人才计划导师天团", " 第 1 章 狗熊会人才计划介绍 在第三期人才计划的线上开营仪式中，熊大分别对狗熊会和人才计划做了详细介绍，权威官方，因此直接照搬来这里。 以下来自熊大 &amp; 水妈。 1.1 狗熊会，what’s that?! 狗熊会？乍一听好像一个黑涩会，这是一个什么鬼组织？如果让我用一两句话去解释，还真的有点困难。如果大家有兴趣，可以在狗熊会公号输入关键词“前世今生”，你会听到一个关于狗熊会来龙去脉的详细解释。简单滴说，狗熊会是一个关注数据产业实践，关注数据科学教育的盈利组织。狗熊会必须是一个盈利组织，因为狗熊会没有任何资金支持，如果自己不能养活自己，第二天就要死翘翘。但是，狗熊会的使命却一点也不盈利，更像一个非盈利组织。狗熊会的使命两句话：聚数据英才、助产业振兴。 第一句“聚数据英才”讲的是狗熊会，之于数据科学教育，的情怀。狗熊会的绝大多数联合创始人都是优秀的老师，他们希望通过狗熊会的平台，让更多的同学（例如：你们），可以享受到更好的数据科学教育。 第二句“助产业振兴”讲的是狗熊会，之于数据科学研究，的理念。狗熊会的绝大多数联合创始人也是优秀的学者，或者是企业家。我们有一个共同的信念，我们不相信，数据科学的真知灼见，能够在办公室里，在笔记本上被敲打出来。我们相信，最优秀的数据科学研究，一定是伴随着数据产业的发展进步，伴随他痛并快乐的成长。这是一个令人激动的进程，我们希望介入其中，而不做旁观者。 1.2 人才计划，OMD! 狗熊会使命的第一句话“聚数据英才”，如何聚？怎么聚？狗熊会有很多种方式。 首先，狗熊会的主要创始人全部都是优秀的老师，我们认真上好每一堂课，给尽可能多的同学传授靠谱的数据科学之，这就是最基础的“聚数据英才的方式”。但是，这样并不能惠及更多的同学。例如，如果大家想听熊大的现场课程，你就必须是北大的学生，这是很挑战的，很不容易。但是，其他的非北大的学生，也想跟熊大学习朴素的数据价值观，肿么办？因此，我们有了第二种方式：数据科学研习社。数据科学研习社是，基于高校的，学生自发组织的，数据科学自学研习组织。狗熊会老师会亲自带大家入门，通过一系列精心设计的TASK，帮助大家培养自主学习的习惯，确定学习研究的良好方式。 所以，这是研习社，是狗熊会“聚数据英才”的第二种方式。但是，这还不够。研习社要求很低，基本上只要同学认真努力，你就可以毕业了。对于部分自我要求更高的同学，需要一种更加严格，更加挑战的学习经历，这就是“人才计划”。研习社以学校为单位，集体形式参加。而人才计划全靠个人。所以，这是狗熊会最看重的数据科学人才培养计划，而且是100%的公益项目。我希望通过这个计划的学习，“聚数据英才，助产业振兴”的理念，能够像DNA一样，也跟植入你们的DNA。伴随你们的学习进步，幸福成长。 更具体而言，我们对人才计划的合格毕业生有三点预期。 第一、你要好学。请你记住，人才计划 什么都给不了你 。给不了你文凭，给不了你奖金。为你能给你的是：学习知识的机会。所以，只有非常纯粹的好学之心，才能让你坚持下来。 第二、你要努力。狗熊会人才计划是每2-3天一个艰巨的TASK会布置下来。而且， 没人教你 怎么做，请help yourself。狗熊会人才计划的目标是最优秀的学生，最优秀的学生，显然不是 教 出来的，都是自己 练 出来的。所以，狗熊会的整个教育理念都是 练 ，而不是 教 。你要非常努力，这里的学习挑战强度，会远远超出你平时的课程学习。 第三、你要谦卑。狗熊会坚持一个最朴素的教育理念。你是学生，就好好当学生，不许评价老师。这样，老师才敢用最严格的标准要求你。所以，在人才计划学习，被批评是常态，被表扬是变态（很少）。因此，你需要一个强大的内心，面对批评，面对挫折，面对挑战，面对失败，面对被淘汰。 对以上三点任何一点有不同看法的同学，可以立刻离开，我们不讨论。 1.3 人才计划导师天团 熊大，北大光华教授，狗熊会创始人，爱数据爱回归！ 水妈，中央财经大学副教授，人才计划的发起人，人称灭绝师太！ 若暄，狗熊会全职教师，狗熊会研习社负责人，互联网广告方面专家。 政委，西安交通大学副教授，狗熊会颜值担当。研究专长机器学习。 媛子，厦门大学副教授，狗熊会女神，擅长多元统计分析等领域的研究和教学。 小丫，人民大学副教授，高颜值高智商高情商，个人征信领域专家。 静静，人民大学助理教授，全世界都想她，社交网络分析领域的专家。 菲菲，人民大学助理教授，温柔善良，文本分析领域的专家。 雪姨，复旦大学助理教授，人美辈分高，小说读的好。 昱姐，北大光华在读博士，擅长打麻将！ "],
["wind.html", "第 2 章 人才计划章程 2.1 纪律 2.2 禁止抄袭 2.3 作业质量 2.4 自学", " 第 2 章 人才计划章程 2.1 纪律 DDL 熊学院布置的所有作业任务，都有明确的 DDL。除特殊原因提前请假外， 到 DDL 不提交作业或者指定资料的学员，一律【淘汰】！ 礼貌 熊学院的任何邮件沟通，在邮件内容里面要包含：自我称呼、对方的 称呼、发邮件的目的等。发微信也是如此。对老师的尊重是最起码的礼貌！但凡 发现缺乏礼貌的学员，一律【劝退】！ 作业 所有的作业任务，都有明确的要求（作业的内容和形式等）。不按照 作业要求提交的学员，1 次以上，一律【劝退】！ 退出 如遇特殊情况中途退出，需提前说明。私自退群或者销声匿迹者，将 被狗熊会列入【黑名单】。 2.2 禁止抄袭 熊学院的 TASK，会明确规定独立完成还是小组合作。需要独立完成的 TASK， 不得参考抄袭他人作业，否则，抄袭者和被抄袭者直接【淘汰】！ 书写报告时，禁止从网络上、参考书上，直接对内容进行复制粘贴。如果需 要参考，必须注明出处。否则，一经发现，一律【劝退】！ 2.3 作业质量 熊学院的 TASK，具有一定的难度。学员提交的作业，老师会认真批改，等级 分为 A、B、C 和 D（等级 D 说明忍无可忍）！两次获得等级【D】，直接【劝退】！ 作业反馈：熊学院布置的作业任务，老师会给出批改意见，需根据意见认真 修改，定期反馈。2 次反馈不能达标的，一律【劝退】！ 2.4 自学 在熊学院学习期间，所有内容以【自学】 为主。老师会提供自学的材料， 学生需按照要求进行自学。每周有固定答疑时间，可以邮件进行提问。非答疑时 间，勿私下经常给老师发微信提问。 在作业过程中遇到的问题，需先独立通过公开渠道搜索，并通过自学查找解 决方案。当通过公开渠道获取答案未果时，再向老师求助或者公开讨论。 熊学院人才计划 于2018年07月15日 "],
["background.html", "第 3 章 TASK 1 背景介绍 3.1 学习资料 3.2 Good Work", " 第 3 章 TASK 1 背景介绍 3.1 学习资料 无论是研宄论文、行业资讯报告还是案例创作，背景介绍尤为重要，因为这部分阐述了进行研宄的动机与意义，是吸引读者继续阅读的源动力。通常一个好的背景介绍包括：行业概述、当前发展状况、存在的问题、研宄目的等。在写背景介绍之前，要先收集足够的关于行业的、业务知识的材料，进行充分的阅读。并且在此基础上，罗列出你要说明的几个要点，并且按照一定的逻辑组织起来。具体地，你可以： 简要地给出关于行业的介绍。 利用权威数字或报告佐证目前行业的发展状况。 分析行业面临哪些问题，聚焦到某一两个点，作为研宄的问题，而非全面地去研宄整个行业。 明确你的研宄问题，指出可能具有的商业价值，未来是否可以实现等。 背景介绍的篇幅不宜过长，以2页左右最为合适。上述的几个要点，可以形成5到6个段落。在组织段落的时候，需要一个行得通的逻辑。读者要练习由面到点地切入研宄问题，先阐述大的行业背景，再层层递进到某个业务问题，最后落脚到研究问题。每个段落的书写，有以下事项需要注意： 每个段落需要一个明确的主旨。可以按照’总分’、’分总’或者’递进’的逻辑进行书写。可以将“主旨句”标粗，方便阅读者迅速了解每个段落的中心思想。 每个段落的篇幅，以10行左右为宜。如果段落太短（比如2行），只有一两句话，说明缺乏深入的讨论；如果段落太长，容易引起阅读疲劳，缺乏重点。过渡的段落，可以篇幅略短。 段落之间，需要一定的衔接，即“承上启下”的句子。不然仅仅只是简单的罗列，非常影响背景介绍的整体逻辑。 想写好一份背景介绍，绝非易事。需要大量的阅读积累和反复练习。背景介绍的书写，常出现以下问题: 段落零碎，东拼西凑。有的段落非常零散，一句话一个自然段，段落之间毫无衔接，没有任何承上启下的语句，而是硬生生拼在一起。相反，有的段落十分冗长，一句话写了10行还不结束，让人找不到重点。 观点全面但毫无逻辑。背景介绍的内容非常全面，定义、国内外现状、存在的问题、未来的发展等依次罗列。每个部分着墨均匀但是毫无重点。如果把段落顺序调换，依然通顺，说明背景介绍缺乏层层推进的逻辑。 文字书写极其不规范。正式的报告中，不要频繁出现类似“我们”，“然后”这种囗语。提交报告之前，仔细阅读，尽量避免错别字的出现。 相关数字的引用，仅仅是为了堆砌数字，而非为了某一观点服务。各种来源的数据、图表胡乱堆一下，凑半页，质量很差。有的数据时效性差，甚至停留在几年之前。 人才计划 2018年7月16日 3.2 Good Work 老师给出的优秀范例，供参考。 随着国民经济的快速增长，人们的文化需求也与日俱增，娱乐活动逐渐多样化。在众多活动中，由于电影成本较低、时间灵活、受众广泛，给身心带来放松的同时，也可以作为朋友、家人增进感情的一种方式，故而得到了众人的青睐。随着越来越多的人走进电影院、国家进行政策扶持，电影行业也在摸索中不断完善自身体系，前景光明。国家新闻出版广电总局电影局数据显示，2017 年我国观影人次达 16.2 亿，比前年 13.72 亿增长了 18.08%。国内电影票房更是从 2012 年的 170.7 亿元增长到 2017年 559.11 亿元，年均复合增长率达到 26.78%。近年来，中国电影每年都有新的票房纪录诞生：2015 年《捉妖记》票房 24.4 亿；2016 年《美人鱼》33.9 亿；2017 年《战狼 2》更是造就了 56.83 亿的票房神话。 从这些国产电影取得的超高票房来看，似乎中国电影产业开启了一次崛起之旅。但是对于普通的电影来说，情况又如何？投资者手握资金，面对天花乱坠的策划，投向哪个 IP 才能赚钱？拿到资金的制作人，拍什么样的电影才能名利双收？ 提到投资和制作，事先对电影行业的运作流程进行了解是必不可少的。中国电影产业链的核心环节主要包括电影的投资（出品）、制作、发行、放映（院线、影院）。一般来讲，出品方筹集影片的制片费用，并支付给制片方；由制片方负责影片的拍摄和后期制作；在影片制作完成之后，宣发机构负责影片在全国范围内的宣传和发行，并向各大院线供应影片；影院获得数字硬盘后，按照其自身及所属院线公司的排映计划对影片进行放映，并提供其他观影服务。以最近热映的《我不是药神》为例，它由坏猴子影业、真乐道文化传播有限公司、欢喜传媒以及北京文化等共同制作出品，北京文化和淘票票联合发行。 有时各部门未必完全分割，为了更好地完成电影的创作及发售，部门间存在合作、共同完成一项工作的情况。比如在《我不是药神》中，出品、宣发几乎全程参与了影片的制作，最后也确实取得了优异的票房成绩。但同时，国内也有着很多“裸奔”项目，投资人把钱直接给制片人和导演，他们自由进行拍摄。但由于投资人对电影市场和电影的艺术价值缺乏认知，他们在分析电影时常常将其简化为卡司阵容和商业元素的堆砌，使得大量烂片仍然能够立项，并获得大投资启动，但最终结果往往可想而知：被舆论指责的同时颗粒无收。另外一种情况是，片子拍到一半出现突发情况，拍摄无法进行，最后甚至直接流产，血本无归。简而言之，投资人为了避免被行业内的一些有心之人“忽悠入坑”从而出现以上损失，在投资之前对影片的口碑、成绩进行预估是十分有必要的。 但是电影从拍摄到放映的整个过程具有非常大的不确定性。在成片出现之前，投资人能做的，只是凭借他对市场和消费者的了解，也就是商业嗅觉；以及对电影表现出的艺术价值的判断，也就是艺术素养；加上一些历史上同类型电影的投资案例，来推断这次投资的成败概率。因此，判断一部电影是否值得投资这个看似简单的问题，其实要求投资者对多方面都有所了解：一方面是市场，比如观众的偏好和市场总体的趋势变化，再比如各种商业元素组合的效果，以及某一具体影片包含的卡司、档期、情节、制作阵容等实际属性的商业价值；一方面是艺术，包括导演艺术、剧作艺术、表演艺术、配乐艺术等；另一方面是数据，即历史上同类电影的成败之中有何可取之处和前车之鉴。 对于其中涉及到的艺术问题，大多要看投资者个人的艺术修养和素质，这是可遇而不可求的特质，有时甚至可以说是看运气。但对电影市场和观众的了解、对某影片进行商业价值预估，以及对造成相似电影成功或失败的因素进行分析判断等，还是有迹可循的。那么具体到实际产业中，这些踪迹在哪里呢？除了投资方以外，制片方在选材、确定拍摄之前，也一定有类似的困惑。比如一部《战狼 2》狂揽票房 56 亿，突破了中国电影单片票房的天花板，那整体动作剧情片的平均成绩也是如此吗？泰印进口的电影近年来也获得了不错的成绩，制片地区的影响力又如何？一些成本不高，但艺术性浓厚的小众电影成了黑马，投资回报率甚至高于大“IP”，是否说明放弃刻意追求话题度反而能带来意想不到的收获？ 以上的探索都是为了创造、寻找一部好的电影，那么究竟什么决定了一部电影在人们心中的较高地位，反应在数据上，就是什么因素可以带来一个较高的分数。作为投资人、制片人最关心的，便是电影最终能取得的成绩。本课题将从电影各项基本反馈数据入手，对影响评分的因素进行探究。这一分析简单易行，且对新片的拍摄、投资人投资、院线购买及排片都具有重要的意义。 陈宛璐 中国人民大学 "],
["data.html", "第 4 章 TASK 2 数据说明 4.1 学习资料 4.2 数据概览 4.3 Good Work", " 第 4 章 TASK 2 数据说明 4.1 学习资料 由于现在的数据集都比较大，变量很多，因此，在做数据分析报告的时候，有必要形成一个数据变景说明表，让读者能够一目了然地了解数据情况。一个规范的数据变量说明表应该包含以下几部分内容。 （1）要有表格标题。一般在表的上方，报告中的表格要有标号。 （2）要标注表头。变量说明表的表头不宜过多，一般包括变量类型、变量名称、取值范围、单位、详细信息、备注等。可以灵活调整，并且无须太详尽，给出总括即可。 （3）变量的归纳分组。中文报告尽量以中文命名。如果有因变量和自变量，需要标明。自变量的展示要根据内容进行归纳分组。 （4）备注说明。数据变量说明表也可以发挥描述分析的作用，为后面的统计分析分担一部分工作。 4.2 数据概览 library(readxl) task_3_data &lt;- read_excel(&quot;task_3_data.xlsx&quot;) #读入数据 summary(task_3_data) #变量概览 ## 公司名称 城市 ## Length:1046 Length:1046 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## ## 贷款金额 期限 月供 ## Length:1046 Length:1046 Min. : 8333 ## Class :character Class :character 1st Qu.: 9333 ## Mode :character Mode :character Median :10133 ## Mean : 9984 ## 3rd Qu.:10633 ## Max. :12633 ## ## 还款总费用 月管理费 期限最低范围 ## Min. :0.00 Length:1046 Min. : 1.00 ## 1st Qu.:1.22 Class :character 1st Qu.: 3.00 ## Median :2.16 Mode :character Median :12.00 ## Mean :1.99 Mean : 8.36 ## 3rd Qu.:2.76 3rd Qu.:12.00 ## Max. :5.16 Max. :12.00 ## ## 期限最高范围 还款方式 放款日期 ## Min. : 12.0 Length:1046 Length:1046 ## 1st Qu.: 36.0 Class :character Class :character ## Median : 36.0 Mode :character Mode :character ## Mean : 36.7 ## 3rd Qu.: 36.0 ## Max. :360.0 ## ## 审批时间 担保方式 申请人数 ## Min. : 0 Length:1046 Min. : 0 ## 1st Qu.: 1 Class :character 1st Qu.: 0 ## Median : 2 Mode :character Median : 33 ## Mean : 2 Mean : 1201 ## 3rd Qu.: 2 3rd Qu.: 462 ## Max. :21 Max. :62898 ## NA&#39;s :2 ## 申请条件 ## Length:1046 ## Class :character ## Mode :character ## ## ## ## 4.3 Good Work 老师给出的优秀范例，供参考。 消费金融是指消费金融公司向各阶层消费者提供消费贷款的现代金融服务方式，而消费金融公司是指以小额、分散为原则，为中国境内居民个人提供以消费为目的的贷款的非银行金融机构。在中国当前的经济环境下，传统银行难以全面发展个人信贷业务，而建立消费金融体系可以有效提升居民的消费水平，支持经济增长。 本课题数据为1046家消费金融公司所发放的，贷款金额为10万元、还款期限为12月的消费贷款明细。数据包含12个变量，分为因变量和自变量两大类。因变量为贷款申请人数，表征该公司贷款的收益与受众规模。自变量分为借贷变量与还贷变量两部分。借贷变量由城市等7个变量组成，其中审批时间是指贷款申请提交到审批通过所需要的工作日，放款日期是指从审批通过到发放贷款所需要的工作日，申请条件山各消费金融公司自行制定，主要关注贷款人的基本信息与资产状况。还贷变量包括月供在内的四个变量，其中还款总费用表示12个月还款结束时贷款人所应还的利息总和。月管理费变量由三种类型和它们各自的取值组成，分别为月管理费、参考月利率和一次性收费，分别表示三种利息计算方式：月管理费是指每个月收取贷款金额的一定比例的费用，月利率指每个月收取当前贷款余额的一定比例的费用，一次性收费是指在贷款时一次性缴纳总利息，在之后的月供中不再需要缴纳利息。具体变量见消费金融数据说明表（表格1）。 黄筝 中国人民大学 "],
["code.html", "第 5 章 TASK 3 代码规范 5.1 学习资料 5.2 共享单车案例", " 第 5 章 TASK 3 代码规范 5.1 学习资料 5.1.1 注释篇 文件注释：写清楚这个代码文件主要是用来做什么的。 分块注释：写清楚这部分代码的主要功能。例如：这部分代码是数据描述的，那部分代码是建模的。 重要的代码注释：写清楚这句代码的主要用途，如何思考的，涉及到的重要公式有时候都可能需要。记住对于每据的代码尽量对齐。 5.1.2 命名篇 用有意义的词定义变量名。例如车联网的案例中，我们会定义某辆车在某段行程中的最大速度。那么就可以使用speed-max来定义这个变量。千万不要使用zuidasudu。 构建自己一致的代码风格。我们命名规则分为：文件名，变量名，函数名，常数名。例如：文件命名predict.adrevenueR（一看就知道是广告收入预测）；变量及函数命名规则：在R环境下，大小写是敏感的。变量名小写字母，单词间用一分隔；函数名用每个单词用大写字母开头，不用一连接。常数项跟函数一样命名但以小k开头。 变量：avg-clicks 函数名CalculateAvgClicks（函数是个动作，所以可以一般写成动宾结构） 常数项：kConstantName 这样就构建了你自己的代码风格。保持一致，并告诉合作者，大家都能明白了。 全球统一的代码风格。如果不能形成自己的代码风格，或者觉得不知道怎么形成。那么使用别人规定好的代码风格就好了。 Google-Style Hadley-Style 5.1.3 函数模块化 代码写得好不好，全靠函数来提高。这什么情况呢？我们继续以车联网的案例为例子。在对车联网数据做数据描述的时候，我们经常希望能够画出某辆车在重要的行程上的轨迹。这里面就会涉及到，如何画从某个启示点到终点的轨迹的代码。设想你画了三个轨迹之后，必然发现代码十分冗余，每次都写同样的画图部分，如果用ggplot2，更是一堆参数设置，不停的复制黏贴。那么为什么我们不把重复出现的代码模块化到同一个函数呢？命名为：PlotCarTrip。 5.1.4 纠错与调试 纠错应该是写好代码最重要的一个环节。我曾经问过写代码的人，你怎么纠错呀。很多人回答是我不纠错！吓死宝宝了，那代码真没问题吗？ 俗话说的好：谁写的代码没有错。知错能改，善莫大焉。那么怎么改！大家尝试下如下按钮。 千万不要掉以轻心，还有两大难关需要过。分别是：提高代码效率与提高代码可读性。 首先，如何检查代码效率呢。代码效率就是要知道，我写了这么多代码，哪部分运行起来最耗时呢？同样的工作，我跑了24小时，累死机器宝宝了。别人1小时跑完，还出去吃喝玩乐了一把，好羡慕呀。那么这问题咋解决。你必须找到你的代码不行的地方。怎么找？可以试试如下按钮哦。 其次，如何解决那些效率低下的代码呢。常见的方法如下： 向量化处理 # 构建一个10*10的矩阵，矩阵的第一列都是1，第二列都是2，一直到第十列都是10 # 方法一 m &lt;- matrix(1, nrow = 10, ncol = 10) for (i in 1:10){ for (j in 1:10){ m[i,j] &lt;- j } } # 方法二：利用了向量化赋值 for (i in 1:10){ m[,i] &lt;- i } # 方法三：线性代数的乘法运算 m &lt;- m %*% diag(1:10) 使用内置函数 # 求矩阵m的列均值 avg1 &lt;- c() for (i in 1:10){ avg1[i] &lt;- mean(m[,i]) } avg2 &lt;- apply(m, 2, mean) avg3 &lt;- colMeans(m) 5.2 共享单车案例 案例难度：** 5.2.1 准备工作 案例背景：共享单车轨迹数据，记录了共享单车在何时处于何位置。通过分析共享单车数据，可以为共享单车调度和研究城市交通带来一定的价值。 案例数据提供了某共享单车系统的站点数据（station）和行程数据（trip）。站点数据告诉我们哪里有什么站点，行程数据告诉我们在何时何地、何人借了何车，又在何时何地还车。为了更好的理解数据，需要了解站点规模和用户类型两个概念。站点规模，即该站点的车桩数。出于经济方面的考虑，规模小的公共自行车系统（O’Brien, O., et al., 2013, p. 3）往往会用规模小的自行车站点。规模大的站点有利于满足公众的骑行需求，应对短时间内的大流量交易；而规模较小的站点适用于鲜有流量大幅激增的区域，降低投资和运营成本，节省公共用地。用户类别分两种，游客和居民。区别是游客的使用是临时的，居民的使用是定期的。详细数据字典可以参加推文公共自行车损坏识别。 这个案例的学习目标包括： 了解共享单车轨迹数据； 理解轨迹数据的基本角度，学会对贡献单车数据做基本的描述性分析，并且适当解读。 了解基本的可视化方法。 5.2.2 分析报告 &amp; 代码 环境设置 ###清除工作环境### rm(list = ls()) ###加载程序包### library(ggplot2) library(plyr) library(lubridate) library(data.table) library(scales) 任务一 找到站点数据Divvy_Stations_2016_Q1Q2和行程数据Divvy_Trips_2016_04。读入样本数据，分别命名为station和trip。 ###读入文件中的数据### station &lt;- read.csv(&quot;Divvy_Stations_2016_Q1Q2.csv&quot;) #站点数据station trip &lt;- read.csv(&quot;Divvy_Trips_2016_04.csv&quot;) #行程数据trip 任务二 将station表的online_date列转换为日期型，将trip表的starttime列和stoptime列分别转换为日期时间型（可用as.POSIXct或lubridate包的mdy_hm函数）。构造三个新列，分别为行程开始时间starttime的小时数，行程结束时间的小时数，行程开始时间增加10分钟得到的时间，分别命名为starttime_h，stoptime_h和t。用summary函数统计数据概况，尝试解读结果，给出你的分析。 ###转换日期数据### station$online_date &lt;- mdy(station$online_date) #station表online_date格式转换 trip$starttime &lt;- mdy_hm(trip$starttime) #trip表starttime和stoptime格式转换 trip$stoptime &lt;- mdy_hm(trip$stoptime) ###添加新的三列### trip &lt;- data.table(trip) trip[,c(&quot;starttime_h&quot;,&quot;stoptime_h&quot;,&quot;t&quot;) := list(hour(starttime),hour(stoptime),starttime + minutes(10))] summary(trip) #查看trip数据结果 ## trip_id starttime ## Min. :9080553 Min. :2016-04-01 00:04:00 ## 1st Qu.:9159400 1st Qu.:2016-04-12 18:14:00 ## Median :9230085 Median :2016-04-17 20:12:00 ## Mean :9230978 Mean :2016-04-17 15:11:03 ## 3rd Qu.:9304310 3rd Qu.:2016-04-23 19:05:00 ## Max. :9379901 Max. :2016-04-30 23:59:00 ## ## stoptime bikeid ## Min. :2016-04-01 00:09:00 Min. : 1 ## 1st Qu.:2016-04-12 18:26:00 1st Qu.:1317 ## Median :2016-04-17 20:39:00 Median :2595 ## Mean :2016-04-17 15:26:46 Mean :2531 ## 3rd Qu.:2016-04-23 19:25:00 3rd Qu.:3773 ## Max. :2016-05-01 14:55:00 Max. :4837 ## ## tripduration from_station_id ## Min. : 60 Min. : 2 ## 1st Qu.: 381 1st Qu.: 75 ## Median : 656 Median :158 ## Mean : 942 Mean :177 ## 3rd Qu.: 1114 3rd Qu.:268 ## Max. :86186 Max. :511 ## ## from_station_name to_station_id ## Streeter Dr &amp; Grand Ave : 3860 Min. : 2 ## Clinton St &amp; Washington Blvd: 3588 1st Qu.: 74 ## Lake Shore Dr &amp; Monroe St : 3176 Median :157 ## Clinton St &amp; Madison St : 2760 Mean :177 ## Theater on the Lake : 2549 3rd Qu.:268 ## Canal St &amp; Adams St : 2486 Max. :511 ## (Other) :213216 ## to_station_name ## Streeter Dr &amp; Grand Ave : 4624 ## Canal St &amp; Madison St : 3205 ## Clinton St &amp; Washington Blvd: 3131 ## Lake Shore Dr &amp; Monroe St : 3039 ## Theater on the Lake : 2672 ## Clinton St &amp; Madison St : 2528 ## (Other) :212436 ## usertype gender birthyear ## Customer : 49392 : 49418 Min. :1899 ## Subscriber:182243 Female: 42947 1st Qu.:1974 ## Male :139270 Median :1983 ## Mean :1980 ## 3rd Qu.:1988 ## Max. :2000 ## NA&#39;s :49392 ## starttime_h stoptime_h ## Min. : 0.0 Min. : 0.0 ## 1st Qu.:10.0 1st Qu.:10.0 ## Median :14.0 Median :15.0 ## Mean :13.6 Mean :13.8 ## 3rd Qu.:17.0 3rd Qu.:17.0 ## Max. :23.0 Max. :23.0 ## ## t ## Min. :2016-04-01 00:14:00 ## 1st Qu.:2016-04-12 18:24:00 ## Median :2016-04-17 20:22:00 ## Mean :2016-04-17 15:21:03 ## 3rd Qu.:2016-04-23 19:15:00 ## Max. :2016-05-01 00:09:00 ## 分析 从summary的结果可以看出，行程开始时间和结束时间的5-numbers较为相近，说明共享单车的借车和换车时间比较集中。 按照计算的小时数来看，均值都在下午一点多，中位数比均值更晚，分别为下午两点和下午三点，因此形成开始和结束的时间都呈现左偏的特征。 下午一点至三点一般是学生上学的时间，也是后半天的工作开始时间，可能有大量的居民使用单车；对于游客来说，吃完午饭开始游玩也正是这个时间，因此使用量较大。 行程时间平均水平为942.5s，中位数小于均值，分布呈现右偏特征，即大多数用户使用共享单车都是短时间的，这也符合共享单车解决“最后一公里”的初衷。 任务三 输出站点规模概况（summary）并绘制直方图。一个共享单车站点可以停几辆单车呢？ ###查看站点规模### summary(station$dpcapacity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 11.0 15.0 15.0 17.2 19.0 47.0 ###绘制站点规模直方图### ggplot(station, aes(x=dpcapacity)) + geom_histogram(binwidth=5, fill=&quot;steelblue4&quot;) + xlim(10, 50) + labs(x = &quot;车桩数&quot;, y = &quot;频数&quot;) 分析 站点规模明显右偏，由于均值容易受异常值影响，因此参考中位数更为科学。由结果可以看出，平均站点规模为一个站点停放15辆共享单车。而一个站点15辆车也符合日常看到的共享单车站点情况。 任务四 请按用户类型统计行程数，输出条形图。这个工作自行车系统，从使用量上来讲，哪个人群是主力呢，贡献了百分之多少的流量呢？ ###统计不同类型用户的行程数### ggplot(trip, aes(x=usertype)) + geom_bar(stat = &quot;count&quot;,fill=&quot;steelblue4&quot;) +scale_x_discrete(name=&#39;用户类型&#39;, labels=c(&#39;游客&#39;,&#39;居民&#39;)) + scale_y_continuous(name=&#39;出行数量&#39;) #绘制直方图 a &lt;- table(trip$usertype)/length(trip$usertype) percent(as.vector(a), accuracy = 0.01) #两类用户贡献流量占比，保留两位小数 ## [1] &quot;21.32%&quot; &quot;78.68%&quot; 分析 这个工作自行车系统，从使用量上来讲，主力是居民，贡献了78.68%的流量。 居民在工作日上下班、上下学的途中会大量使用共享单车，而在周末也会贡献一部分力量；游客更多会在节假日游玩时使用共享单车，因此并不是单车使用的主力。 任务五 自行车的命运是否也有不同呢？统计每辆自行车的使用次数，得到自行车日均使用次数直方图，并尝试解读。提示：平均每辆自行车一个内会会被使用几次呢？最小值，四分之一位数，中位数，四分之三位数，最大值又是多少？从这个角度看，你觉得这个共享单车系统的单车使用率怎么样？高还是低？说说你的分析。 trip[,monthlyuse := length(trip_id), by = &quot;bikeid&quot;] #统计每一辆自行车的月均使用次数（频数） bikeuseid &lt;- unique(trip[,.(bikeid,monthlyuse)]) #保证原数据框不被更改，新建bikeuseid数据框 ###查看每辆自行车月均使用频率### summary(bikeuseid$monthlyuse) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 35 52 51 67 184 ###绘制每辆自行车月均使用次数直方图### ggplot(bikeuseid, aes(x=monthlyuse)) + geom_histogram(binwidth=10, fill=&quot;steelblue4&quot;) + labs(x = &quot;自行车日均使用次数&quot;, y = &quot;频率&quot;) 分析 根据数据结果，平均每一辆自行车一个月内会被使用51.05次，其5-number特征数如上所示。 虽然最高的使用频率是一个月184次，即一天6次，但是一辆共享单车平均每天只会被使用不到2次。从这个角度来看，此共享单车系统的使用率偏低。 任务六 请按天分析一日的时均借车量和还车量并绘制直方图，并解读结果。提示：共享单车的使用是否有早晚高峰，若有，分别是几点呢？ ###绘制时均借车量直方图### trip[,usehour := hour(starttime)] #借车的小时index trip[,hourlyuse := length(trip_id), by = &quot;usehour&quot;] #每个小时借车的数量 startuse &lt;- unique(trip[,.(usehour,hourlyuse)]) #得到绘制条形图的数据框 ggplot(startuse,aes(x = usehour, y = hourlyuse)) + geom_bar(stat=&#39;identity&#39;, fill=&quot;steelblue4&quot;) + labs(x = &quot;使用小时&quot;, y = &quot;借车量&quot;) #绘制时均借车量直方图 ###绘制时均还车量直方图### trip[,backhour := hour(stoptime)] #还车的小时index trip[,hourlyback := length(trip_id), by = &quot;backhour&quot;] #每个小时还车的数量 stopuse &lt;- unique(trip[,.(backhour,hourlyback)]) #得到绘制条形图的数据框 ggplot(stopuse,aes(x = backhour, y = hourlyback)) + geom_bar(stat=&#39;identity&#39;, fill=&quot;steelblue4&quot;) + labs(x = &quot;使用小时&quot;, y = &quot;还车量&quot;) #绘制时均还车量直方图 分析 根据直方图，共享单车的借车和还车都具有早晚高峰。对于借车、还车来说，早高峰都为早上8点，晚高峰都为下午5点，并且晚高峰时段借还车量都大于早高峰。早晚高峰时间正好是上下班、上下学的时间，结果符合预期。 对于早高峰的还车量较多，一个推测是：通常上班族会在临近上班的时间到达目的地，因此还车量在这个时刻会很高，而出发时间取决于其居住地点的远近，因此借车量没有还车量高。 任务七 分用户类型统计周内日均还车量。至此，你应该充分掌握了绘制条形图的方法，尝试将游客和居民的周内日均还车量绘制成条形图。理解不同用户类型的出行特点并解读。提示，工作日使用单车多，还是非工作日多？不同人群一样吗？ ###定义绘制某类用户周内日均还车量直方图的函数### detach(&quot;package:data.table&quot;) #由于&quot;data.table&quot;和&quot;lubridate&quot;有共同的函数，因此临时detach一个包 PlotUserBack &lt;- function(user){ #绘制日均还车量（星期）条形图函数 subtrip &lt;- trip[trip$usertype == user,] #选出不同类型的骑车人 day_in_week &lt;- c() day_in_week &lt;- weekdays(subtrip$stoptime) # day_in_week &lt;- wday(subtrip$stoptime,label = T) #取出还车时间的星期 sub_back_date &lt;- data.frame(date = day_in_week) #设置还车星期数据框 sub_back_date$date = factor(sub_back_date$date, levels=c(&#39;星期一&#39;,&#39;星期二&#39;,&#39;星期三&#39;,&#39;星期四&#39;,&#39;星期五&#39;,&#39;星期六&#39;,&#39;星期日&#39;)) #重新对星期level排序 ggplot(sub_back_date, aes(x=date)) + geom_bar(stat = &quot;count&quot;,fill=&quot;steelblue4&quot;) + labs(x = &quot;星期&quot;, y = &quot;还车量&quot;) #绘制一周内日均还车量直方图 } PlotUserBack(&quot;Subscriber&quot;) #绘制居民日均还车量 PlotUserBack(&quot;Customer&quot;) #绘制游客日均还车量 library(data.table) #再library回刚才的data.table包 分析 还车量可以代表出行数量，由直方图可以看出，不同类型的用户具有不同的出行时间特点。 居民的出行量整体高于游客使用共享单车的出行量，这和居民和用户本身的数量有关。居民在工作日（周一至周五）还车量较多，多数用于上下班和上学、放学；而游客在周六和周日出行较多，多数用于游玩——这表现出不同类型的用户使用共享单车的目的不同。 任务八 统计用户出行时长分布，并计算按出行时长排序后，对应行程数占比的累计值，绘图并分析，这里你将学着分析累计密度图。提示：短行程多还是长行程多？是否有类似二八定律的发现？ ###计算用户出行时长，单位为分钟### trip[,trip_use_time := tripduration/60] #转换出行时长单位为分钟 ###计算出行时长频数### trip_use_time &lt;- table(trip$trip_use_time) #统计频数 trip_use_time &lt;- data.frame(as.numeric(names(trip_use_time)),as.numeric(trip_use_time)) #转换为数据框 colnames(trip_use_time) &lt;- c(&quot;time&quot;,&quot;freq&quot;) #给数据框重命列名 trip_use_time &lt;- trip_use_time[order(trip_use_time$time),] #将频数按从小到大的顺序重新排列 ###计算出行时长累积频率### use_time_freq &lt;- data.frame() accumulate &lt;- 0 for(i in 1:nrow(trip_use_time)){ #计算每一种出行时长的累积频数 accumulate &lt;- accumulate + trip_use_time[i,&quot;freq&quot;] row &lt;- cbind(trip_use_time$time[i],accumulate) #拼接数据框 use_time_freq &lt;- rbind(use_time_freq,row) } totalfreq &lt;- sum(trip_use_time$freq) #总频数 for(i in 1:nrow(trip_use_time)){ #计算每一种出行时长的累计频率 use_time_freq$accu_pro[i] &lt;- use_time_freq$accumulate[i]/totalfreq } colnames(use_time_freq) &lt;- c(&quot;time&quot;,&quot;accu_num&quot;,&quot;accu_pro&quot;) #数据框重命列名 ###统计出行时长的5-numbers### quantile(use_time_freq$time, probs = c(0,0.25,0.5,0.75,1)) ## 0% 25% 50% 75% 100% ## 1.00 28.03 55.25 93.55 1436.43 ###绘制出行时长累积密度曲线### ggplot(use_time_freq) + geom_line(aes(x = time, y = accu_pro)) + labs(x = &quot;出行时长（单位：分钟）&quot;,y = &quot;Pr[X&lt;x]&quot;) 分析 累积分布曲线的斜率是概率密度，因此由图可以看出，行程每增加一分钟，数量增加的越来越少。行程符合“二八定律”，即行程短的数量多，行程较长的数量少。 任务九 这里你将学习如何把两张表的数据关联起来，得到新发现。分别打印出居民女性、居民男性和游客还车量前三的站点及其经纬度。猜猜这些站点流量大的原因是什么？提示：你可以借助网络，了解下这些站点及其地理位置。 ###选出三种用户类型对应的trip子数据框### sub_female &lt;- trip[((trip$usertype == &quot;Subscriber&quot;)&amp;(trip$gender == &quot;Female&quot;)),] #女性居民 sub_male &lt;- trip[((trip$usertype == &quot;Subscriber&quot;)&amp;(trip$gender == &quot;Male&quot;)),] #男性居民 customer &lt;- trip[trip$usertype == &quot;Customer&quot;,] #游客 ###定义输出还车量前三的站点及其经纬度函数### Top3Station &lt;- function(usertype){ #usertype可以选择为sub_female, sub_male, customer usertype[,cnt := length(trip_id), by = &quot;to_station_id&quot;] #计算每个还车站点流量 to_station &lt;- unique(usertype[,c(&quot;to_station_id&quot;,&quot;to_station_name&quot;,&quot;cnt&quot;)]) #每个站点的流量数据框 for (j in 1:nrow(to_station)){ #关联trip表和station表 to_station$latitude[j] &lt;- station[which(station$id == to_station$to_station_id[j]),&quot;latitude&quot;] #按照station ID连接两张表，获取纬度 to_station$longitude[j] &lt;- station[which(station$id == to_station$to_station_id[j]),&quot;longitude&quot;] #按照station ID连接两张表，获取经度 } to_station &lt;- arrange(to_station,-cnt) #按照流量由大到小排序 to_station[c(1:3),c(2:5)] #输出流量前三的站点信息 } ###打印出居民女性、居民男性和游客还车量前三的站点及其经纬度### Top3Station(sub_female) #女性居民Top3流量站点 ## to_station_name cnt latitude longitude ## 1 Dearborn St &amp; Erie St 382 41.89 -87.63 ## 2 Wabash Ave &amp; Roosevelt Rd 379 41.87 -87.63 ## 3 Kingsbury St &amp; Kinzie St 378 41.89 -87.64 Top3Station(sub_male) #男性居民Top3流量站点 ## to_station_name cnt latitude longitude ## 1 Canal St &amp; Madison St 2718 41.88 -87.64 ## 2 Clinton St &amp; Washington Blvd 2713 41.88 -87.64 ## 3 Clinton St &amp; Madison St 2062 41.88 -87.64 Top3Station(customer) #游客Top3流量站点 ## to_station_name cnt latitude longitude ## 1 Streeter Dr &amp; Grand Ave 3654 41.89 -87.61 ## 2 Lake Shore Dr &amp; Monroe St 2485 41.88 -87.62 ## 3 Lake Shore Dr &amp; North Blvd 1651 41.91 -87.63 分析 女性居民Top3站点 Top 1: Dearborn St &amp; Erie St Top 2: Wabash Ave &amp; Roosevelt Rd Top 3: Kingsbury St &amp; Kinzie St 由Google地图定位可以看出，女性居民最常使用共享单车的目的地杂货店、餐馆和健身俱乐部。 男性居民Top3站点 Top 1: Canal St &amp; Madison St Top 2: Clinton St &amp; Washington Blvd Top 3: Clinton St &amp; Madison St 而男性居民最常见的还车点则集中在地铁站、公交车站等交通运输中心。 5.2.2.1 游客Top3站点 Top 1: Streeter Dr &amp; Grand Ave Top 2: Lake Shore Dr &amp; Monroe St Top 3: Lake Shore Dr &amp; North Blvd 与居民不同，游客的共享单车还车点集中在博物馆、公园、码头以及美术馆，这体现了游客和居民使用共享单车的不同目的。 "],
["descriptive.html", "第 6 章 TASK 4 描述分析 6.1 学习资料 6.2 题目 6.3 探究影响消费金融贷款申请人数的因素——描述性分析 6.4 代码", " 第 6 章 TASK 4 描述分析 6.1 学习资料 描述分析是数据分析报告当中非常重要的环节。描述分析的主要内容包括： 用统计图对数据进行初步的展示。统计图是最能吸引读者的工具，能够给人留下深刻印象，做好了能为报告或者展示大大加分。 用统计表以及各种统计指标对数据进行描述。有的时候，并不适合用统计图展示数据，那么统计表（如频数分布表）或者简单的统计指标（如均值、标准差等）也是很好的选择。 适当地解读描述的结果。描述分析的重点，在于对统计图表的解读。单独展示统计图表，并没有太大的意义。根据统计图表“讲故事”，从统计图表中发现问题才是描述分析的真正目的。 描述分析的整体规范，需要注意篇幅、排版和逻辑三个方面的事项。 篇幅。如今的数据，指标非常丰富，动辄上百个变量。如果一个变量绘制一个统计图，那么将会轻松地完成一本“丑图集锦”！这个时候，就不能在报告中展示所有的变量的描述分析结果，而是要有所取舍。一份长度适中（10页左右）的数据分析报告里面，描述分析的篇幅在3页左右比较合适。 排版。描述分析部分的排版，容易出现的问题是图表尺寸太大，或者一页报告全是图表没有文字。在排版的时候，尽量不要一个图挨着一个图，而是统计图和描述性的文字穿插进行。注意，也不要出现大篇幅的留白。 逻辑。学会对变量进行归纳分组，非常重要。将能够归纳成组的变量，整合在一起作图汇报，而非一个个单独进行描述。 更多地关于统计图的规范，相信大家在阅读过图百讲系列之后己经非常清楚了。接下来说说描述性文字的撰写。 描述性文字的撰写，可以分为两个层次。第一个层次叫做客观陈述，即描述统计图所展现的现象。比如直方图的分布形态、柱状图中各个类别的频数多少等。这个层次的描述性文字，相对容易，主要是做到用语准确，尤其是跟统计学相关的术语。第二个层次叫做合理推断，即解读统计图背后的原因，推测数据为什么呈现出某种规律。这个层次的描述性文字相对较难，需要撰写者进行深入的思考，给出合理的解释。对于初学者来说，通过大量的练习能够比较容易地实现第一个层次的提高。至于第二个层次，需要多接触各行各业的数据以及了解业务问题，才有可能更加合理挖掘数据背后的故事。 6.2 题目 这个TASK将进入一个关键的节点，数据的描述。实际上，很多数据分析报告甚至是实际项目，都仅仅依赖于描述性分析。 在这个TASK任务中，你要照常先下载相关材料并进行仔细阅读。 然后，利用TASK 3的数据完成这个任务。具体地： 你已经对TASK 3的数据有一定的了解。选择一个合适的因变量，并且定义清楚研究问题。 你无需再书写大段的背景介绍，用一个段落说清楚背景和研究问题，然后直接进入描述分析的环节。与此同时，你也无需再介绍数据了，不要复制粘贴你的变量说明表。 请认真组织你的描述分析部分，做到逻辑清晰。你可以设计小标题，方便老师们阅读批改。 用R或者是python完成这个任务，不要用EXCEL完成。把你的代码整理好，写注释，因为我（水妈）也许会抽风抽查。 注意，提交PDF文件，篇幅是3页。依然是小四号宋体，1.5倍行距。请做到图文穿插。统计图也不要太大个头。这次，请你写清楚题目。 DDL：2018年7月26日晚24:00. 请认真完成这个作业，尤其注意你的描述性文字。这个TASK过后，人才计划会进行一轮淘汰。 6.3 探究影响消费金融贷款申请人数的因素——描述性分析 某银行的董事长兼CEO老王发现最近消费金融的发展如火如荼，于是也计划在自己的公司开展提供消费贷款的服务，以吸引更多的客户，提高本公司收益规模。但是提供什么样的贷款才能吸引更多的申请客户呢？老王为此十分着急。就在此时，得力助手小张获取了1046家消费金融公司所发放的，贷款金额为10万元、还款期限为12月的消费贷款明细，包括审批时间、担保方式等7个借贷变量，月供、还款总费用等4个还贷变量，以及贷款申请人数变量。老王在对数据进行深入了解后，凭借大学掌握的统计学知识开展了描述性分析，看看什么因素对贷款的申请人数有影响。 6.3.1 对申请人数（因变量）进行描述 由直方图（图1）可以看出，申请人数呈现明显的右偏分布。具体而言，申请人数的均值为1201人，中位数为33人，最大值为62898人，最小值为0人。由于均值容易受异常值影响，因此均值与中位数相差较大是由于有几个申请人数非常多的贷款产品存在。这在老王的预料范围之内——相当出色的贷款产品是少数，也存在很多没有申请人数的贷款产品。因此老王推测，在形形色色的贷款产品中，大多数人都会选择市面上口碑较好、贷款人数较多的，从而导致有少数极端热门贷款和部分无人申请贷款的局面出现。鉴于数据明显右偏，后续的直方图分类比较中，老王做了取对数的处理，从而使得对比结果更直观。 6.3.2 对借贷变量（自变量）进行描述 老王的公司具有全国各地分公司，但是应该在哪一家分公司发行消费贷款产品呢？他对不同城市贷款产品的申请人数绘制了直方图。 由图2看出，济南的申请人数平均水平（中位数）最低，重庆的最高，除了这两个城市之外，其他10个城市平均水平（中位数）和波动范围的差异比较不明显。其中上海和深圳的平均水平较高，贷款规模较大的数量也比较多，老王认为这是两所城市消费水平较高所导致的；对于北京的贷款规模平平，老王查阅资料后发现消费金融是不能经营“车贷”和“房贷”的，而北京市民更多负担房贷，消费金融水平自然就不高了；成都、重庆、杭州和西安四所城市的贷款规模较高，他推测这四所新一线城市的经济快速增长带来了消费金融的繁荣。 从借入贷款的角度考虑，贷款人提出申请到发放贷款之间等待的总时长是否会影响贷款规模呢？通过条形图（图3），老王发现间隔时间为3-4天的贷款产品申请人数最多，随着等待时间的增加，申请人数整体呈现降低趋势。他猜想，间隔时间太短，可能会给客户留下审批程序不严格的不良印象，而时间太长又会引起客户的不耐烦，因此等待时长3-4天为最佳。 担保方式不同的贷款产品会不会对客户的申请借款意愿产生影响呢？老王通过箱线图发现，“担保贷”竟然是一条直线！原来是因为“担保贷”的数据中申请人数非零的只有一条。其他三种担保方式中，信用贷的申请人数平均水平（中位数）明显高于“抵押贷”和“自由选”，说明凭借借款人信誉，无需提供抵押品或第三方担保的贷款更受客户青睐。 6.3.3 对还贷变量（自变量）进行描述 从还款过程来看，月管理费（图5）收取的是每个月的利息，而“一次性收费”的贷款产品申请人数全为0，因此老王没有将这一种放在比较范围内。箱线图显示，“参考月利率”方式的平均申请人数高于“月管理费”方式，说明客户更喜欢申请按照贷款余额的一定比例计算利息，老王推测，由于贷款余额是可调整的，客户会更偏好按照不同时期个人经济状况来调整月管理费水平。 三种还款方式中（图6），老王发现分期还款的贷款产品申请人数最多，他推测进行消费借贷的客户一般是经济状况不够好的，而到期还款和随借随还的还贷压力较大，分期还款更适合这一类贷款人。 6.3.4 初步结论 经过上述描述分析，老王得出了初步的结论：选择上海、广州、深圳三大一线城市和重庆、西安、成都、杭州四个一线城市的分公司，发行审批时间与放款时间合计3-4天、按照参考月利率收取每月利息的分期还款信用贷，可以吸引更大规模的申请人数，促进公司发展。 6.4 代码 ###加载需要的程序包### library(ggplot2) library(data.table) library(readxl) ###读入数据文件### task_3_data &lt;- read_excel(&quot;task_3_data.xlsx&quot;) ###更改列名### colnames(task_3_data) &lt;- c(&quot;company&quot;,&quot;city&quot;,&quot;loan&quot;,&quot;deadline&quot;,&quot;monthly_pay&quot;,&quot;total_fee&quot;,&quot;monthly_fee&quot;,&quot;ddl_lower&quot;,&quot;ddl_upper&quot;,&quot;way_to_pay&quot;,&quot;get_money&quot;,&quot;get_permmsion&quot;,&quot;way_to_insure&quot;,&quot;apply&quot;,&quot;condition&quot;) ###绘图theme函数### my_ggtheme &lt;- function(){ theme(axis.title.x = element_text(size = 16))+ theme(axis.title.y = element_text(size = 16))+ theme(axis.text.x = element_text(size = 16))+ theme(axis.text.y = element_text(size = 16)) } ###因变量分布### ggplot(task_3_data, aes(x=apply)) + geom_bar(stat=&quot;bin&quot;, fill=&quot;steelblue4&quot; ) + labs(x = &quot;申请人数&quot;, y = &quot;频数&quot;)+ my_ggtheme() ###将管理费类型和费率分别解析出来### split_fee &lt;- as.data.frame(tstrsplit(task_3_data$monthly_fee, &quot; &quot;)) #根据中间的空格拆分两列信息，设置为数据框split_fee colnames(split_fee) &lt;- c(&quot;fee_type&quot;,&quot;fee_rate&quot;) #重新给列起名 task_3_data &lt;- cbind(task_3_data, split_fee) #和原数据框合并 ###城市分类箱线图### data_no_zero &lt;- task_3_data[-which(task_3_data$apply == 0),] #箱线图取对数画，去掉apply为0的 data_no_zero &lt;- data.table(data_no_zero) data_no_zero[,med_apply := median(apply), by = &quot;city&quot;] #计算每个城市申请人数的中位数 ggplot(data_no_zero, aes(x = city,y=log(apply)))+ #绘制箱线图 geom_boxplot(aes(fill=city))+ geom_point(data=data_no_zero,aes(x=reorder(city, med_apply),y=log(med_apply)),shape=15,size=2)+ geom_line(data=data_no_zero,aes(x=city,y=log(med_apply),group = 1),color = &quot;black&quot;,size=1,linetype = 1)+ labs(x = &quot;城市&quot;,y = &quot;申请人数&quot;, fill = &quot;城市&quot;)+ my_ggtheme() ###绘制担保方式分类箱线图### ggplot(data_no_zero, aes(x = way_to_insure,y=log(apply)))+ geom_boxplot(aes(fill=way_to_insure))+ labs(x = &quot;担保方式&quot;,y = &quot;申请人数&quot;, fill = &quot;担保方式&quot;)+ my_ggtheme() ###还款方式箱线图### ggplot(data_no_zero, aes(x = way_to_pay,y=log(apply)))+ geom_boxplot(aes(fill=way_to_pay))+ labs(x = &quot;还款方式&quot;,y = &quot;申请人数&quot;, fill = &quot;还款方式&quot;)+ my_ggtheme() ###日期条形图### unique(task_3_data$get_money) #查看等待时长都有哪些情况 task_3_data[which(task_3_data$get_money == &quot;审批后当日（审批为3&quot;),&quot;get_money&quot;] &lt;- 0 #将文字改为意思相同的数字 task_3_data$get_money &lt;- as.numeric(task_3_data$get_money) #转化为数值型 task_3_data &lt;- data.table(task_3_data) #转化为data.table task_3_data$time_before &lt;- task_3_data$get_money + task_3_data$get_permmsion #计算拿到贷款前的等待时间总长 ggplot(task_3_data) + #绘制条形图 geom_bar(aes(x=time_before, y=apply/10000),stat = &quot;identity&quot;)+ labs(x = &quot;审批时间+放款时间&quot;, y = &quot;总申请人数/万人&quot;)+ my_ggtheme() #########################根据图来看，这两个变量和申请人数似乎没有相关关系 ###月供散点图### ggplot(data_no_zero, aes(x = monthly_pay,y = log(apply),color = log(apply)))+ geom_point(size=3)+ geom_smooth(method = &#39;lm&#39;)+ labs(x = &quot;月供/元&quot;,y = &quot;申请人数(取对数)&quot;,color = &quot;申请人数&quot;) ###还款总费用散点图### ggplot(data_no_zero, aes(x = total_fee,y = log(apply),color = log(apply)))+ geom_point(size=3)+ geom_smooth(method = &#39;lm&#39;)+ labs(x = &quot;还款总费用/万元&quot;,y = &quot;申请人数(取对数)&quot;,color = &quot;申请人数&quot;) ######################### ###月管理费### #绘制费用类型箱线图 #注：一次性收费类型申请人数全是0 ggplot(data_no_zero, aes(x = fee_type,y=log(apply)))+ geom_boxplot(aes(fill=fee_type))+ labs(x = &quot;月管理费类型&quot;,y = &quot;申请人数&quot;, fill = &quot;月管理费类型&quot;)+ my_ggtheme() "],
["linear-model.html", "第 7 章 TASK 5 线性回归分析 7.1 学习资料 7.2 短租房热度案例", " 第 7 章 TASK 5 线性回归分析 7.1 学习资料 大家好，从这个任务开始，进入到建模的工作当中。任务的难度开始增加，自学的成分也更重了，我不再提供学习材料。你应该已经习惯高强度的自学，以及摸索出了自己的学习途径。关于线性回归的知识点和案例，参考公众号熊大的数据价值回归系列，以及精品案例的推文。 这个TASK是我们上一期的优秀学员完成的，我希望你们做得更好，具体要求。 下载数据和任务文档。 完成任务文档里面的所有题目，注意解读，注意代码规范和注释！ 排版整齐即可，没有特殊要求。 提交HTML文件，由于熊学堂无法直接上传HTML文件，请先压缩再上传压缩文档。 DDL：2018年7月30日晚24:00. 7.2 短租房热度案例 案例难度：*** 7.2.1 准备工作 案例背景：短租房是当前火热的“共享经济”的一种，房主将闲置的房屋、房间布置装修，并以按天计费的方式提供短期出租服务。短租房性价比高、个性化、居家感浓厚，已经成为人们出行住宿的新选择，特别是对年轻人和短期游客，短租越来越受到他们的青睐。短租房数据通过对短租平台爬取得到，包括短租房的价格、面积、位置、软硬件条件、评分、评论信息等。其中短租房得到的评论的多少可以用来衡量短租房的热度。通过对短租房数据的统计分析，可以了解短租房的热度受到哪些因素的影响，进而为优化短租房的经营、提升短租房的热度提供参考信息。 案例数据提供了一批短租房的数据，包括：从网页爬取的短租房基本信息、地理位置信息，以及退房规定、房屋配置、房屋得分等。详细的案例介绍请查看案例推文 短租房热度影响因素分析。 这个案例的学习目标包括： 了解短租房数据； 学会对于短租房的数据做适当描述分析，并进行结果解读； 掌握变量构造的思路； 用回归分析寻找短租房热度的影响因素； 以下为数据中的变量及其介绍： 变量名 变量介绍 vol_num 短租房评论数 price 价格 deposit 押金 area 面积 capacity 宜住人数 bed 床数 room 房间数 office 客厅数 toilet 厕所数 kitchen 厨房数 balcony 阳台数 history 累积上线时间（单位：月） book_type 出租类型 latitude 纬度 longitude 经度 district_with_outer 城区（四环外各区合并为“其他”） is_foreigner 是否接待外国人 is_cook 是否允许做饭 is_party 是否允许聚会 is_smoke 是否允许抽烟 is_parking 是否有停车位 is_security 是否有门禁系统 is_elevator 是否有电梯 is_wifi 是否有wifi is_washing 是否有洗衣机 is_refrigerator 是否有冰箱 is_soap 是否提供肥皂 is_drinkingwater 是否有饮水机 is_heating 是否有暖气 is_aircondition 是否有空调 is_tv 是否有电视 is_hotwater 是否有热水 toilet_type 是否为独立厕所 rule_n 入住前多少天可退订 rule_a 退订需要扣除前多少天的定金 otherscore_clean 房主其他房源清洁卫生得分 otherscore_description 房主其他房源描述相符得分 otherscore_trans 房主其他房源交通得分 otherscore_safe 房主其他房源安全得分 otherscore_performance 房主其他房源性价比得分 subway_num 附近地铁数量（定性变量，水平为：“3个以上”、“3个以下”） hospital_num 附近是否有医院 college_num 附近是否有学校 scenic_spot_num 附近是否有景点 price_landmark_mean 同商圈短租房平均价格 vol_landmark_mean 同商圈短租房平均评论数 vol_other_landmark_mean 同商圈房主平均其他房源评论数 sp2 同商圈房屋中该短租房价格排名的比例 sp4 同商圈房屋中该短租房面积排名的比例 sp6 同商圈房屋中该短租房清洁卫生得分排名的比例 sp8 同商圈房屋中该短租房描述相符得分排名的比例 sp10 同商圈房屋中该短租房交通得分排名的比例 sp12 同商圈房屋中该短租房安全得分排名的比例 sp14 同商圈房屋中该短租房性价比得分排名的比例 sp16 同商圈房屋中该短租房价格面积比排名的比例 附近定义为：2500m内 商圈为通过百度地图接口，根据经纬度定位确定房屋所属的商圈。 7.2.2 分析报告 &amp; 代码 环境设置 ###清除工作环境### rm(list = ls()) ###安装需要的程序包### devtools::install_github( &quot;ricardo-bion/ggradar&quot;, dependencies= TRUE) ###加载程序包### library(ggplot2) library(lubridate) library(plyr) library(ggradar) library(data.table) 任务一 找到数据data。读入样本数据（提前设置好工作路径），命名为dat0。用summary函数查看数据记录的变量等情况。将样本量定义为n，也就是观测数。 setwd(&quot;D:/Melody_Ren/onedrive/Rworkspace/bearclub&quot;) #设置工作路径 dat0 &lt;- read.csv(&quot;data.csv&quot;) #读入文件中的数据并命名 dat1 &lt;- dat0 #对原始数据备份，用于分析 summary(dat0) #查看数据记录的变量等情况 ## house_id vol_num price ## Min. :1.10e+07 Min. : 1.0 Min. : 39 ## 1st Qu.:2.92e+09 1st Qu.: 3.0 1st Qu.: 188 ## Median :6.32e+09 Median : 6.0 Median : 299 ## Mean :7.72e+09 Mean : 12.3 Mean : 355 ## 3rd Qu.:1.21e+10 3rd Qu.: 15.0 3rd Qu.: 428 ## Max. :2.06e+10 Max. :219.0 Max. :10000 ## deposit area capacity ## Min. : 0 Min. : 4.0 Min. : 1.00 ## 1st Qu.: 200 1st Qu.: 18.0 1st Qu.: 2.00 ## Median : 300 Median : 45.0 Median : 2.00 ## Mean : 346 Mean : 49.6 Mean : 2.99 ## 3rd Qu.: 500 3rd Qu.: 65.0 3rd Qu.: 4.00 ## Max. :7000 Max. :750.0 Max. :10.00 ## bed room office ## Min. : 1.00 Min. : 0.00 Min. :0.00 ## 1st Qu.: 1.00 1st Qu.: 1.00 1st Qu.:1.00 ## Median : 1.00 Median : 2.00 Median :1.00 ## Mean : 1.72 Mean : 2.09 Mean :1.01 ## 3rd Qu.: 2.00 3rd Qu.: 3.00 3rd Qu.:1.00 ## Max. :18.00 Max. :17.00 Max. :6.00 ## toilet kitchen balcony ## Min. : 0.00 Min. : 0.000 Min. :0.000 ## 1st Qu.: 1.00 1st Qu.: 1.000 1st Qu.:1.000 ## Median : 1.00 Median : 1.000 Median :1.000 ## Mean : 1.21 Mean : 0.951 Mean :0.949 ## 3rd Qu.: 1.00 3rd Qu.: 1.000 3rd Qu.:1.000 ## Max. :17.00 Max. :11.000 Max. :6.000 ## history book_type latitude ## Min. : 1.0 床位出租: 115 Min. :39.6 ## 1st Qu.: 3.0 独立单间:1327 1st Qu.:39.9 ## Median : 7.0 沙发出租: 38 Median :39.9 ## Mean :10.8 整套出租:2331 Mean :39.9 ## 3rd Qu.:15.0 3rd Qu.:40.0 ## Max. :53.0 Max. :40.4 ## longitude district_with_outer is_foreigner ## Min. :116 朝阳区:1666 Min. :0.000 ## 1st Qu.:116 东城区: 371 1st Qu.:0.000 ## Median :116 丰台区: 341 Median :1.000 ## Mean :116 海淀区: 642 Mean :0.634 ## 3rd Qu.:116 其他 : 489 3rd Qu.:1.000 ## Max. :117 西城区: 302 Max. :1.000 ## is_cook is_wifi is_party ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:0.000 ## Median :1.000 Median :1.000 Median :0.000 ## Mean :0.809 Mean :0.994 Mean :0.305 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.000 Max. :1.000 Max. :1.000 ## is_washing is_refrigerator is_smoke ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:0.000 ## Median :1.000 Median :1.000 Median :0.000 ## Mean :0.958 Mean :0.965 Mean :0.429 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.000 Max. :1.000 Max. :1.000 ## is_parking is_security is_elevator ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:0.000 1st Qu.:0.000 1st Qu.:0.000 ## Median :0.000 Median :1.000 Median :1.000 ## Mean :0.469 Mean :0.695 Mean :0.727 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.000 Max. :1.000 Max. :1.000 ## is_soap is_drinkingwater is_heating ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:0.000 1st Qu.:0.000 1st Qu.:1.000 ## Median :1.000 Median :1.000 Median :1.000 ## Mean :0.738 Mean :0.745 Mean :0.965 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.000 Max. :1.000 Max. :1.000 ## is_aircondition is_tv is_hotwater ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:1.000 ## Median :1.000 Median :1.000 Median :1.000 ## Mean :0.967 Mean :0.813 Mean :0.996 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.000 Max. :1.000 Max. :1.000 ## toilet_type rule_n rule_a ## Min. :0.000 Min. :1.00 Min. :1.00 ## 1st Qu.:0.000 1st Qu.:1.00 1st Qu.:1.00 ## Median :1.000 Median :3.00 Median :1.00 ## Mean :0.655 Mean :3.54 Mean :2.74 ## 3rd Qu.:1.000 3rd Qu.:7.00 3rd Qu.:4.00 ## Max. :1.000 Max. :7.00 Max. :7.00 ## otherscore_clean otherscore_description ## Min. :3.00 Min. :3.60 ## 1st Qu.:4.80 1st Qu.:4.80 ## Median :4.90 Median :4.90 ## Mean :4.87 Mean :4.88 ## 3rd Qu.:5.00 3rd Qu.:5.00 ## Max. :5.00 Max. :5.00 ## otherscore_trans otherscore_safe ## Min. :3.30 Min. :3.60 ## 1st Qu.:4.80 1st Qu.:4.90 ## Median :4.90 Median :5.00 ## Mean :4.88 Mean :4.92 ## 3rd Qu.:5.00 3rd Qu.:5.00 ## Max. :5.00 Max. :5.00 ## otherscore_performance subway_num hospital_num ## Min. :3.00 3个以上:3381 无: 5 ## 1st Qu.:4.80 3个以下: 430 有:3806 ## Median :4.90 ## Mean :4.85 ## 3rd Qu.:5.00 ## Max. :5.00 ## college_num scenic_spot_num price_landmark_mean ## 无: 81 无: 7 Min. : 88.7 ## 有:3730 有:3804 1st Qu.: 319.2 ## Median : 362.2 ## Mean : 370.7 ## 3rd Qu.: 402.5 ## Max. :2163.6 ## vol_landmark_mean vol_other_landmark_mean ## Min. : 0.00 Min. : 0.0 ## 1st Qu.: 5.83 1st Qu.: 30.6 ## Median : 7.04 Median : 41.5 ## Mean : 7.50 Mean : 49.0 ## 3rd Qu.: 8.93 3rd Qu.: 66.7 ## Max. :15.69 Max. :167.7 ## sp2 sp4 sp6 ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:0.194 1st Qu.:0.257 1st Qu.:0.000 ## Median :0.440 Median :0.526 Median :0.531 ## Mean :0.456 Mean :0.515 Mean :0.430 ## 3rd Qu.:0.703 3rd Qu.:0.786 3rd Qu.:0.700 ## Max. :0.994 Max. :0.995 Max. :0.992 ## sp8 sp10 sp12 ## Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:0.000 1st Qu.:0.000 1st Qu.:0.000 ## Median :0.556 Median :0.571 Median :0.625 ## Mean :0.442 Mean :0.439 Mean :0.466 ## 3rd Qu.:0.710 3rd Qu.:0.722 3rd Qu.:0.745 ## Max. :0.992 Max. :0.992 Max. :0.988 ## sp14 sp16 ## Min. :0.000 Min. :0.000 ## 1st Qu.:0.000 1st Qu.:0.454 ## Median :0.500 Median :0.537 ## Mean :0.418 Mean :0.522 ## 3rd Qu.:0.712 3rd Qu.:0.593 ## Max. :0.988 Max. :0.981 n &lt;- nrow(dat0) #观测数据数量n n #查看n值 ## [1] 3811 任务二 绘制短租房评论数的直方图，通过这个任务，你可以了解短租房评论的分布情况。 ###设置画图theme### my_ggtheme &lt;- function(){ theme(axis.title.x = element_text(size = 16))+ theme(axis.title.y = element_text(size = 16))+ theme(axis.text.x = element_text(size = 16))+ theme(axis.text.y = element_text(size = 16)) } ###绘制短租房评论数的直方图### ggplot(dat1, aes(x = vol_num)) + geom_histogram(binwidth = 10, fill=&quot;lightblue&quot;, color = &quot;black&quot;) + labs(x = &quot;评论数&quot;, y = &quot;频数&quot;) + my_ggtheme() 描述与解读 短租房的评论数量呈现明显的右偏分布。具体地，只有23.3%的短租房超过15人评论，门庭冷落的短租房较多，约有67.3%的短租房评论数不超过10条，因此大多数短租房的热度都偏低。其中，评论数最多（219条）的短租房是位于北京市西城区月坛街道真武庙社区的整套出租房，其附近有3个以上的地铁站，并且还有医院、学校和景点，可谓是短租房中的战斗机。 任务三 分别绘制价格-评论数与面积-评论数对比箱线图，分析短租房的价格和面积与短租房热度之间的关系。 dat1 &lt;- data.table(dat1) dat1[,price_flag := &quot;小于或等于300元&quot;] dat1[price &gt; 300, price_flag := &quot;大于300元&quot;] #以300元为分界线，对价格分类 dat1[,area_flag := &quot;小于或等于50平米&quot;] dat1[area &gt; 50, area_flag := &quot;大于50平米&quot;] #以50平米为分界线，对面积分类 ###绘制价格-评论数对比箱线图### dat1$price_flag = factor(dat1$price_flag, levels=c(&quot;小于或等于300元&quot;,&quot;大于300元&quot;)) #调整将价格的排序 ggplot(dat1, aes(x = price_flag, y = log(vol_num), group = price_flag)) + #绘制箱线图 geom_boxplot(fill = c(&quot;lightblue&quot;,&quot;lightpink&quot;)) + #设置箱子填充颜色 labs(x = &quot;价格&quot;,y = &quot;对数评论数&quot;) + #坐标轴名称 my_ggtheme() ###绘制面积-评论数对比箱线图### dat1$area_flag = factor(dat1$area_flag, levels=c(&quot;小于或等于50平米&quot;,&quot;大于50平米&quot;)) #调整面积的排序 ggplot(dat1, aes(x = area_flag, y = log(vol_num), group = area_flag)) + #绘制箱线图 geom_boxplot(fill = c(&quot;lightblue&quot;,&quot;lightpink&quot;)) + #设置箱子填充颜色 labs(x = &quot;面积&quot;,y = &quot;对数评论数&quot;) + #坐标轴名称 my_ggtheme() 描述与解读 从分组箱线图可以看出，不同价格的短租房和不同面积的短租房热度有一定的差异。其中，价格小于或等于300元和面积小于或等于50平米的的短租房热度更高，一种合理的推测是：短租房的需求更多来自于外地居民，由于较为迫切的需求，导致其对面积的要求不高，并且年轻人的经济实力不是很强，因此“便宜又小巧”成为短租房中更受青睐的种类。 任务四 统计各城区（四环外其他各区合并记为“其他”）的平均评论数，及短租房数量，并绘制城区-平均评论数的柱状图。 plot_district &lt;- as.data.frame(aggregate(dat1$vol_num, list(dat1$district_with_outer), mean)) #按城区分组统计热度均值 colnames(plot_district) &lt;- c(&quot;district_name&quot;,&quot;ave_price&quot;) #给数据框重命列名 ggplot(plot_district) + #绘制城区-平均评论数的柱状图 geom_bar(aes(x = reorder(district_name, -ave_price), y = ave_price), #绘制条形图 stat = &quot;identity&quot;, fill = &quot;lightblue&quot;, color = &quot;black&quot;)+ #按照柱状图高低排序 geom_text(aes(x = district_name, y = ave_price, label = round(ave_price,1)), vjust = -0.3)+ #添加数据标签 labs(x = &quot;&quot;, y = &quot;平均评论数&quot;, caption = &quot;（样本数量：341、642、302、1666、371、489）&quot;) + #添加坐标轴名称和副标题 my_ggtheme() 描述与解读 通过条形图可以看出，虽然西城区的短租房数量最多，但是丰台区的房源热度（评价数量）最多，其他地区的房源热度最低。看来虽然西城区更靠近北京市中心，终究也是敌不过丰台区性价比更高的短租房来得“讨人欢心”。 任务五 以房主其他房源的性价比得分、描述相符得分为例，绘制对比柱状图来探索这些变量和平均评论数之间的关系。 dat1[,c(&quot;other_clean_flag&quot;,&quot;other_disc_flag&quot;, &quot;other_trans_flag&quot;,&quot;other_safe_flag&quot;,&quot;other_perf_flag&quot;) := c(&quot;小于或等于4.8分&quot;)] dat1[otherscore_clean &gt; 4.8, other_clean_flag := &quot;大于4.8分&quot;] dat1[otherscore_description &gt; 4.8, other_disc_flag := &quot;大于4.8分&quot;] dat1[otherscore_trans &gt; 4.8, other_trans_flag := &quot;大于4.8分&quot;] dat1[otherscore_safe &gt; 4.8, other_safe_flag := &quot;大于4.8分&quot;] dat1[otherscore_performance &gt; 4.8, other_perf_flag := &quot;大于4.8分&quot;] PlotOtherSource &lt;- function(other_source_flag){ #定义“绘制其他房源得分”函数，输入值为得分方面 other_flag &lt;- dat1[,mean(vol_num), by = get(other_source_flag)] #按照得分高低统计热度均值 colnames(other_flag) &lt;- c(&quot;other_factor&quot;,&quot;ave_vol_num&quot;) #给数据框重命列名 other_flag$other_factor = factor(other_flag$other_factor, levels=c(&quot;小于或等于4.8分&quot;,&quot;大于4.8分&quot;)) #调整得分的排序 ggplot(other_flag, aes(x = other_factor ,y = ave_vol_num,fill = other_factor)) + #绘制得分-平均评论数柱状图 geom_bar(stat = &#39;identity&#39;, fill = c(&quot;lightblue&quot;,&quot;lightpink&quot;), color = &quot;black&quot;) + #设置颜色 labs(x = &quot;&quot;, y = &quot;平均评论数&quot;, #坐标轴名称 caption = paste(&quot;（样本数量：&quot;, #副标题名称，计算样本量 nrow(dat1[get(other_source_flag) == &quot;小于或等于4.8分&quot;]), &quot;、&quot;, nrow(dat1[get(other_source_flag) == &quot;大于4.8分&quot;]), &quot;）&quot;)) + geom_text(aes(x = other_factor, y = ave_vol_num, label = round(ave_vol_num,1), vjust = -0.5)) + #添加数据标签 my_ggtheme() } PlotOtherSource(&quot;other_disc_flag&quot;) #绘制描述得分-平均评论数柱状图 PlotOtherSource(&quot;other_perf_flag&quot;) #绘制性价比得分-平均评论数柱状图 描述与解读 通过两个柱状图可以看出，对其他房源表述相符得分和性价比得分方面，都是高分（大于4.8分）样本数量较多，而房主其他房源高分的短租房热度也高于其他房源低分的短租房。这说明大多数房主经营的租住房在性价比和描述相符方面都是较为优秀的，并且这会具有良性循环的效果，“你好我好大家好”，其他房源评价高，那这间房屋也不差。 任务六 建立回归模型来分析短租房评论数和各影响因素之间的关系，观察最终得到的回归系数并尝试解释对系数进行解释。 ###建立回归模型### tem_house &lt;- dat0 #拷贝原始数据作为建模数据 tem_house$log_price &lt;- log(tem_house$price) #对价格取对数 tem_house$log_history &lt;- log(tem_house$history) #对累积在线时长取对数 tem_house &lt;- tem_house[,-3] #在模型数据中删除原始价格数据 tem_house &lt;- tem_house[,-12] #删除原始在线时长数据 tem_house &lt;- tem_house[,-1] #删除短租房ID lm.full &lt;- lm(log(vol_num) ~ ., data=tem_house) #建立全模型，因变量为对数评论数，自变量为其他所有变量 lm.null &lt;- lm(log(vol_num) ~ 1,data=tem_house) #建立空模型，因变量为对数评论数，不包括任何自变量 lm.bic &lt;- step(lm.full,k=log(n),trace=F) #使用BIC准则选择模型 summary(lm.bic) #观察最终得到的回归系数 ## ## Call: ## lm(formula = log(vol_num) ~ area + book_type + is_smoke + is_parking + ## is_drinkingwater + rule_n + rule_a + otherscore_clean + otherscore_description + ## price_landmark_mean + vol_landmark_mean + sp6 + sp8 + sp10 + ## sp12 + sp16 + log_price + log_history, data = tem_house) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8871 -0.4414 -0.0467 0.4314 2.3243 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) -3.567264 0.379302 -9.40 ## area 0.001091 0.000360 3.03 ## book_type独立单间 0.024818 0.065899 0.38 ## book_type沙发出租 -0.212920 0.119676 -1.78 ## book_type整套出租 0.303494 0.072467 4.19 ## is_smoke -0.073270 0.022206 -3.30 ## is_parking -0.064830 0.021559 -3.01 ## is_drinkingwater 0.079655 0.024705 3.22 ## rule_n 0.031557 0.005814 5.43 ## rule_a -0.018085 0.006306 -2.87 ## otherscore_clean 0.379932 0.101212 3.75 ## otherscore_description 0.609519 0.111464 5.47 ## price_landmark_mean 0.000428 0.000108 3.95 ## vol_landmark_mean 0.035380 0.004169 8.49 ## sp6 0.483276 0.077611 6.23 ## sp8 0.435986 0.084961 5.13 ## sp10 0.548505 0.066734 8.22 ## sp12 0.528108 0.078227 6.75 ## sp16 -0.352273 0.086484 -4.07 ## log_price -0.302287 0.031722 -9.53 ## log_history 0.429846 0.012848 33.46 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## area 0.00249 ** ## book_type独立单间 0.70649 ## book_type沙发出租 0.07530 . ## book_type整套出租 2.9e-05 *** ## is_smoke 0.00098 *** ## is_parking 0.00265 ** ## is_drinkingwater 0.00127 ** ## rule_n 6.1e-08 *** ## rule_a 0.00416 ** ## otherscore_clean 0.00018 *** ## otherscore_description 4.8e-08 *** ## price_landmark_mean 7.8e-05 *** ## vol_landmark_mean &lt; 2e-16 *** ## sp6 5.3e-10 *** ## sp8 3.0e-07 *** ## sp10 2.8e-16 *** ## sp12 1.7e-11 *** ## sp16 4.7e-05 *** ## log_price &lt; 2e-16 *** ## log_history &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.638 on 3790 degrees of freedom ## Multiple R-squared: 0.706, Adjusted R-squared: 0.705 ## F-statistic: 455 on 20 and 3790 DF, p-value: &lt;2e-16 描述与解读 经过变量选择后，得到最终的对数线性回归模型。根据模型的系数，在控制其他因素不变时，可以得到如下结论： 短租房的基本信息中，价格越低，面积越大，评论数越多； 房屋的累计上线时间每多一个月，则热度会提升0.43条； 对于出租类型这一变量，沙发出租的最低，整套出租的房屋热度最高，比床位出租短租房的评论数量每间高出0.3条； 在房屋配置中，有饮水机的热度最高，其次是允许抽烟的房屋，有停车位的热度更低； 有关退房规定，入住前可退定限制天数越多，租房热度越高，退订扣除的定金越多，租房热度越低； 对于房主其他房源评分这一变量，描述相符评分每高出1分，则评论数会增加0.6，其次，干净卫生评分每高1分，评论数增加0.38条； 同商圈房屋整体平均热度越高，该短租房的热度越高； 在同商圈房屋中，该短租房在卫生得分、描述相符得分、交通得分、安全得分中排名越高，其热度越高，价格面积比排名越低，热度也会越高。 这些结论与之前的猜想基本符合，而且模型的F检验拒绝原假设，说明建立的模型是显著的；调整的\\(R^2\\)为0.7045，模型的拟合程度较好。 任务七 使用任务六中得到的回归模型预测某房屋半年后收获的评论数，结果为13条。 predict_set &lt;- data.frame(area = 17, book_type = &quot;独立单间&quot;, #将该短租房18个自变量设置为数据框 is_smoke = 0, is_parking = 0, is_drinkingwater = 0, rule_n = 4,rule_a = 3, otherscore_clean = 4.7, otherscore_description = 4.9, price_landmark_mean = 10, vol_landmark_mean = 350, sp6 = 0.2, sp8 = 0.2, sp10 = 0.2, sp12 = 0.2, sp16 = 0.9, log_history = log(6), log_price = log(200)) predict_vol_num &lt;- predict(lm.bic, predict_set) #使用task6中的模型对评论数做预测 round(predict_vol_num,0) #将预测值四舍五入得到评论数 ## 1 ## 13 任务八 基于任务七中的短租房的数据绘制雷达图，根据以下几个方面描述该短租房的优势、劣势，对短租房的优化布置提供建议。 coeff &lt;- as.data.frame(t(as.matrix(lm.bic$coefficients))) #回归系数数据框 coeff_to_percentile &lt;- coeff[,c(2,5:21)] #在出租类型只取一列，数据框列数等于自变量数量 GetPercentile &lt;- function(variable_name){ #定义获取相对位置（percentile）的函数，输入值为自变量名称 a &lt;- ecdf(tem_house[,variable_name]) #累积概率密度 t &lt;- a(predict_set[,variable_name]) return(t) #返回值为输入变量在全部数据中的排名相对位置 } variable_percentile &lt;- c() #变量相对位置向量 for (i in c(1,2:18)){ #除去出租类型，计算其他变量排名相对位置 if(coeff_to_percentile[1,i] &gt; 0) #回归系数为正，取累计概率密度；反之，取1-累计概率密度 variable_percentile[i] &lt;- GetPercentile(colnames(predict_set)[i]) else variable_percentile[i] &lt;- 1-GetPercentile(colnames(predict_set)[i]) } var_percentile &lt;- as.data.frame(t(as.matrix(variable_percentile))) #相对位置向量设置为数据框 colnames(var_percentile) &lt;- colnames(predict_set) #取列名 ###基本信息得分### basic_info &lt;- coeff_to_percentile[,c(&quot;area&quot;,&quot;log_price&quot;)] #取回归系数为权重 basic_info_score &lt;- sum(var_percentile$area*basic_info$area, #对第一个维度下的变量相对位置加权 var_percentile$log_price*basic_info$log_price) ###房屋配置得分### house_allo &lt;- coeff_to_percentile[,c(&quot;is_smoke&quot;,&quot;is_parking&quot;,&quot;is_drinkingwater&quot;)] house_allo_score &lt;- sum(var_percentile$is_smoke*house_allo$is_smoke, var_percentile$is_parking*house_allo$is_parking, var_percentile$is_drinkingwater*house_allo$is_drinkingwater) ###退房规定得分### check_out &lt;- coeff_to_percentile[,c(&quot;rule_n&quot;,&quot;rule_a&quot;)] check_out_score &lt;- sum(var_percentile$rule_a*check_out$rule_a, var_percentile$rule_n*check_out$rule_n) ###其他房源评分### other_source &lt;- coeff_to_percentile[,c(&quot;otherscore_clean&quot;,&quot;otherscore_description&quot;)] other_source_score &lt;- sum(var_percentile$otherscore_clean*other_source$otherscore_clean, var_percentile$otherscore_description*other_source$otherscore_description) ###同商圈房屋整体情况### landmark &lt;- coeff_to_percentile[,c(&quot;price_landmark_mean&quot;,&quot;vol_landmark_mean&quot;)] landmark_score &lt;- sum(var_percentile$price_landmark_mean*landmark$price_landmark_mean, var_percentile$vol_landmark_mean*landmark$vol_landmark_mean) ###同商圈房屋比较### sp &lt;- coeff_to_percentile[,c(&quot;sp6&quot;,&quot;sp8&quot;,&quot;sp10&quot;,&quot;sp12&quot;,&quot;sp16&quot;)] sp_score &lt;- sum(var_percentile$sp6*sp$sp6, var_percentile$sp8*sp$sp8, var_percentile$sp10*sp$sp10, var_percentile$sp12*sp$sp12, var_percentile$sp16*sp$sp16) ###出租类型得分### #出租类型的系数排名为：整套出租&gt;独立单间&gt;床位出租&gt;沙发出租 book_type_score &lt;- 1 - nrow(tem_house[which(tem_house$book_type == &quot;整套出租&quot;),])/n #出租类型的相对位置 ###将7个维度下的得分放入rardarplot数据框 rardarplot &lt;- data.frame(V1 = &quot;维度&quot;,&quot;基本信息&quot; = basic_info_score, &quot;房屋配置&quot; = house_allo_score, &quot;退房规定&quot; = check_out_score, &quot;其他房源&quot; = other_source_score, &quot;商圈整体&quot; = landmark_score, &quot;商圈房屋&quot; = sp_score, &quot;出租类型&quot; = book_type_score) ###绘制雷达图### ggradar(rardarplot, grid.label.size = 5, axis.label.size = 5, group.point.size = 4, plot.legend = FALSE)+ theme(legend.position = &quot;none&quot;) 说明 每个维度得分的计算方法： 对于出租类型，按照每种取值的回归系数大小排序后，计算其相对位置的累计概率密度； 对于除去出租类型的其他维度，以该维度下的每个自变量在原数据集对应变量的相对位置为被加权值，以每个自变量的回归系数为权重，进行加权，得到得分。 描述与解读 该短租房在同商圈房屋卫生得分、描述相符得分、交通得分、安全得分、价格面积比中排名占有较大优势，房主的其他房源评分也是一大优势。然而，在同商圈房屋平均价格、平均评论数两个方面较为劣势，“独立单间”的出租类型也降低该出租房的竞争力；此外，退房规定也是该房屋评论数上升的潜在威胁。 根据该房屋雷达图和任务七中的系数解读，为提高该短租房的热度（评论数），对该房主提出几点建议： 继续保持其在同商圈房屋各方面得分的排名； 继续保持其他房源评分水平； 对照同商圈房屋的平均评分，适当降低该房屋的出租价格； 将出租类型调整为“整套出租”； 尽量增加入住前可提前退订天数，减少退订扣除的定金。 7.2.3 标准答案代码 标准答案和上述分析报告中附上的代码差别主要在最后一部分雷达图的计算方法上。 ######准备工作###### # 清除工作环境 cat(&#39;\\014&#39;) rm(list = ls()) ######任务一###### dat0 = read.csv(&quot;data.csv&quot;,header = T, fileEncoding = &quot;gbk&quot;) #读入样本数据 summary(dat0) #查看数据的基本情况 ######任务二###### # 绘制短租房评论数直方图 hist( dat0$vol_num, #该列为短租房评论数 main = NULL, #设置为无标题 breaks = 20, #设置条形数量为20 xlab = &quot;评论数&quot;, #x轴标题为评论数 col = &quot;#FEBF00&quot;, #设置颜色 ylab = &quot;频数&quot; #y轴标题为频数 ) ######任务三###### # 价格-评论数 price_0 &lt;- subset(dat0, dat0$price &lt;= 300)$vol_num #提取小于等于300元的短租房的评论数 price_1 &lt;- subset(dat0, dat0$price &gt; 300)$vol_num #提取大于300元的短租房的评论数 # 绘制对比箱线图。短租房评论数极差较大，作图效果较差，于是采用对数评论数作图 boxplot( main = NULL, #设置为无标题 varwidth = T, #设置箱体宽度与样本数量相关 col = c(&quot;#A5A5A5&quot;, &quot;#FEBF00&quot;), #设置颜色 ylab = &quot;对数评论数&quot;, #y轴标题为对数评论数 names = c(&quot;小于或等于300元&quot;, &quot;大于300元&quot;), #设置箱体名称 log(price_0), #对小于等于300元的短租房评论数取对数 log(price_1) #对大于300元的短租房评论数取对数 ) # 面积-评论数 area_0 &lt;- subset(dat0, dat0$area &lt;= 50)$vol_num #提取小于等于50平米的短租房的评论数 area_1 &lt;- subset(dat0, dat0$area &gt; 50)$vol_num #提取大于50平米的短租房的评论数 # 绘制对比箱线图。短租房评论数极差较大，作图效果较差，于是采用对数评论数作图 boxplot( main = NULL, #设置为无标题 varwidth = T, #设置箱体宽度与样本数量相关 col = c(&quot;#A5A5A5&quot;, &quot;#FEBF00&quot;), #设置颜色 ylab = &quot;对数评论数&quot;, #y轴标题为对数评论数 names = c(&quot;小于或等于50平米&quot;, &quot;大于50平米&quot;), #设置箱体名称 log(area_0), #对小于等于50平米的短租房评论数取对数 log(area_1) #对大于50平米的短租房评论数取对数 ) ######任务四###### stat &lt;- tapply(dat0$vol_num, dat0$district_with_outer, mean) #统计每个城区的短租房平均评论数 stat_order &lt;- order(stat, decreasing = T) #获得城区短租房平均评论数从大到小的顺序 stat &lt;- stat[stat_order] #将统计结果按照平均评论数大小从大到小排序 counts &lt;- table(dat0$district_with_outer)[stat_order] # 统计样本数量 # 绘制柱状图 p &lt;- barplot( stat, #统计结果 ylim = c(0, 17), #设置y轴显示范围 main = NULL, #设置为无标题 col = &quot;#FEBF00&quot;, #设置颜色 ylab = &quot;平均评论数&quot;, #设置y轴标题为平均评论数 names = c(&quot;丰台区&quot;, &quot;海淀区&quot;, &quot;西城区&quot;, &quot;朝阳区&quot;, &quot;东城区&quot;, &quot;其他&quot;), #设置柱体标题 sub = &quot;(样本数量:341、642、302、1666、371、489)&quot; #在副标题位置标注样本量 ) text(p, stat, labels = round(stat, 1), pos = 3) #在图表上显示统计结果数值 ######任务五###### # 房主其他房源性价比得分 counts &lt;- table(dat0$otherscore_performance &gt; 4.8) #对房主其他房源得分，设定阈值为4.8分，统计大于该阈值和小于该阈值的样本量 stat &lt;- tapply(dat0$vol_num, dat0$otherscore_performance &gt; 4.8, mean) #基于该阈值的划分，统计短租房平均评论数 # 绘制对比柱状图 p &lt;- barplot( stat, #统计结果 ylim = c(0, 16), #设置y轴显示范围 main = NULL, #设置为无标题 col = c(&quot;#A5A5A5&quot;, &quot;#FEBF00&quot;), #设置颜色 ylab = &quot;平均评论数&quot;, #设置y轴标题为平均评论数 names = c(&quot;小于或等于4.8分&quot;, &quot;大于4.8分&quot;), #设置柱体标题 sub = &quot;(样本数量：1364、2447)&quot; #在副标题位置标注样本量 ) text(p, stat, labels = round(stat, 1), pos = 3) #在图表上显示结果数值 # 房主其他房源描述相符得分 counts &lt;- table(dat0$otherscore_description &gt; 4.8) #同样设定阈值为4.8分，统计大于该阈值和小于该阈值的样本量 stat &lt;- tapply(dat0$vol_num, dat0$otherscore_description &gt; 4.8, mean) #基于该阈值的划分，统计短租房平均评论数 # 绘制对比柱状图 p &lt;- barplot( stat, #统计结果 ylim = c(0, 16), #设置y轴显示范围 main = NULL, #设置为无标题 col = c(&quot;#A5A5A5&quot;, &quot;#FEBF00&quot;), #设置颜色 ylab = &quot;平均评论数&quot;, #设置y轴标题为平均评论数 names = c(&quot;小于或等于4.8分&quot;, &quot;大于4.8分&quot;), #设置柱体标题 sub = &quot;(样本数量：1088、2723)&quot; #在副标题位置标注样本量 ) text(p, stat, labels = round(stat, 1), pos = 3) #在图表上显示结果数值 ######任务六###### #引入数据处理相关的包 library(dplyr) #对变量进行处理 temp_house &lt;- dat0 %&gt;% mutate( toilet_type = factor(toilet_type), #将厕所类型变量转为factor subway_num = factor(subway_num, levels = c(&quot;3个以下&quot;, &quot;3个以上&quot;)), #将周围地铁站数量变量转为factor rule_a = as.numeric(gsub(&quot;Day&quot;, &quot;&quot;, rule_a)), #去除变量中的字符串，转为数值 rule_n = as.numeric(gsub(&quot;Day&quot;, &quot;&quot;, rule_n)), #去除变量中的字符串，转为数值 book_type = factor(book_type, levels = c(&quot;沙发出租&quot;, &quot;床位出租&quot;, &quot;独立单间&quot;, &quot;整套出租&quot;)), #将出租类型变量转为factor log_history = log(history), #对在线时长进行变换 log_price = log(price) #对价格进行对数变换 ) %&gt;% select( -house_id, -price, -history #去除不属于自变量的房屋ID，以及原始的价格、在线时长变量 ) %&gt;% na.omit() #去除缺失数据 model_lm0 &lt;- lm(log(vol_num) ~ ., data=temp_house) #基于变量处理后的数据建立回归模型 model_lm0_step &lt;- step(model_lm0, k=log(nrow(temp_house)), trace = FALSE) #使用BIC准则选择变量，进行逐步回归 summary(model_lm0_step) #展示最终回归结果 ######任务七###### # 回归分析的应用（预测） # 根据题目描述信息构建一个测试样本 new_data &lt;- data.frame( area = 17, book_type = &quot;独立单间&quot;, is_smoke = 0, rule_n = 4, rule_a = 3, otherscore_clean = 4.7, otherscore_description = 4.9, vol_landmark_mean = 10, price_landmark_mean = 350, sp6 = 0.8, sp8 = 0.8, sp10 = 0.8, sp12 = 0.8, sp16 = 0.1, is_parking = 0, is_drinkingwater = 0, log_history = log(6), log_price = log(200) ) round(exp(predict(model_lm0_step, new_data))) #使用回归模型对测试样本预测，预测半年后获得的评论数，并将预测结果保留为整数 ######任务八###### train_terms_predict &lt;- predict(model_lm0_step, type = &quot;terms&quot;) %&gt;% as.data.frame() #计算模型对训练数据每一个变量的预测值 new_data_terms_predict &lt;- predict(model_lm0_step, new_data, type = &quot;terms&quot;) %&gt;% as.data.frame() #计算模型对新短租房数据每一个变量的预测值 # 根据题目描述信息，将变量按照评价方面分成7个维度 group &lt;- list( 基本信息 = c(&quot;log_price&quot;, &quot;area&quot;), 房屋配置 = c(&quot;is_smoke&quot;, &quot;is_parking&quot;, &quot;is_drinkingwater&quot;), 退房规定 = c(&quot;rule_n&quot;, &quot;rule_a&quot;), 房东其它房源评分 = c(&quot;otherscore_clean&quot;, &quot;otherscore_description&quot;), 同商圈房屋整体情况 = c(&quot;price_landmark_mean&quot;, &quot;vol_landmark_mean&quot;), 同商圈房屋比较 = c(&quot;sp6&quot;, &quot;sp8&quot;, &quot;sp10&quot;, &quot;sp12&quot;, &quot;sp16&quot;), 出租类型 = c(&quot;book_type&quot;) ) # 计算各维度数值取值的范围 dim_range &lt;- sapply(group, function(x) { range(rowSums(train_terms_predict[x])) }) # 计算新短租房在各维度的取值 dim_value &lt;- sapply(group, function(x) { sum(new_data_terms_predict[x]) }) # 计算新短租房在每个维度的得分根据维度范围标准化后结果（雷达图所用数据） radar_data &lt;- (dim_value - dim_range[1, ]) / (dim_range[2, ] - dim_range[1, ]) %&gt;% matrix(nrow = 1) %&gt;% as.data.frame() # 分组名称（用“\\n”指定了显示名称时的换行位置） var_label &lt;- c( &quot;基本信息&quot;, &quot;房屋配置&quot;, &quot;退房规定&quot;, &quot;房东其它\\n房源评分&quot;, &quot;同商圈房屋\\n整体情况&quot;, &quot;同商圈\\n房屋比较&quot;, &quot;出租类型&quot; ) # 引入绘制雷达图需要的包 library(ggradar) # 绘制雷达图 ggradar( cbind(&quot;&quot;, radar_data), #雷达图所用数据 axis.labels = var_label, #设置雷达图各方向的名称 axis.label.size = 4.5, #设置维度名称字体大小 background.circle.colour = &quot;#CDCDCD&quot;, #设置背景颜色 gridline.mid.colour = &quot;#2E75B6&quot;, #设置对应50%的网格线颜色 grid.label.size = 0, #设置不显示网格线注释（即网格线对应的百分比） group.point.size = 5, #设置图中点的尺寸 group.line.width = 1.3 #设置图中连线的宽度 ) + scale_color_manual(breaks = NULL, values = &quot;#FFC000&quot;) + #设置连线的颜色 theme( #设置为背景透明 plot.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.background = element_rect(fill = &quot;transparent&quot;, colour = NA) ) "],
["binomial.html", "第 8 章 TASK 6 分类问题 8.1 学习资料 8.2 题目 8.3 影响消费贷款有效性的因素分析 8.4 代码", " 第 8 章 TASK 6 分类问题 8.1 学习资料 在这个TASK,我们尝试针对离散型因变量建立模型，这属于分类问题。当离散型因变量只有两个可能的取值（例如性别有’男’和’女’这两个取值）时，这就是一个’二分类’问题，也是这个TASK的学习重点。 我们给出更多的二分类问题的例子。在消费者购买决策的研宄中，消费者的决策有两个可能的结果：‘购买’和’不购买’；在病人的癌症诊断过程中，诊断的结果有两个可能的取值：“得癌症”和“不得癌症”；在申请贷款的审批流程中，审批的结果有两个可能的取值：“审批”和“不审批”。 二分类问题的因变量Y，惯例上取值为0和1（0和1只是数字符号，并不支持代数运算）。如果继续沿用线性模型，那么 \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)（假设只有一个自变量）。线性回归模型并非不可行，最小二乘估计依然能够获得。其实，很多实际数据分析经常简单粗暴地采用线性回归模型来处理二分类问题。但是，细心的读者能够发现，理论上这个模型的左右两边并不等价。我们在这个TASK将要介绍两类模型（逻辑回归和决策树），用于解决二分类问题。 8.1.1 逻辑回归模型 逻辑回归模型并不直接对因变量Y进行建模，而是对\\(Y = 1\\)的可能性（概率）建立模型。具体地，\\[P(Y = 1) = \\frac{exp(\\beta_0 + \\beta_1 X)}{1 + exp(\\beta_0 + \\beta_1 X)}\\] 这个函数叫做Logistic函数，所以这个回归模型又叫做逻辑回归。关于逻辑回归，我们需要关注这样几个点。 odds的概念：odds是可能性的一种度量，即：\\(P(Y=1)/P(Y=0)=exp(\\beta_0+\\beta_1 X)\\)。这个量是“因变量为1的概率”与“因变量为0的概率”的比值。 log-odds的概念：log-odds是对odds取对数，这个变换也叫作logit变换，即：\\[log\\frac{P(Y=1)}{P(Y=0)}=\\beta_0+\\beta_1 X\\]这个量，恰好是自变量的线性组合。在线性回归模型当中，自变量的线性组合（加上误3．差项）就是因变量。在逻辑回归模型当中，自变量的线性组合就是log-odds。 系数的估计与解读：逻辑回归的系数估计，不再是最小二乘估计，而是要使用极大似然估计（MLE）。我们不去关心系数估计的理论性质，感兴趣的读者可以阅读广义线性模型的经典书籍。假设己经得到了系数估计，\\(\\hat\\beta_1=3\\)，常见的有这样三个层面的解读。 只关心系数估计的正和负。如果系数估计为正，说明相应的自变量的增加（控制其他因素不变），会导致Y=1的可能性的增加：相反，如果系数估计为负，说明相应的自变量的增加（控制其他因素不变），会导致Y=0的可能性的增加。当然，系数估计必须显著（相应的p值小于给定的显著性水平），我们才对其进行解读。 关心odds的变化。之前介绍了什么是odds，以此为基础，还有odds ratio，即两个odds的比：\\[\\frac{P(Y&#39;=1)}{P(Y&#39;=0)} \\frac{P(Y=0)}{P(Y=1)}=\\frac{exp(\\beta_0+\\beta_1 X&#39;)}{exp(\\beta_0+\\beta_1 X)}=exp(\\beta_1 X&#39;-\\beta_1X)\\]那么，\\(\\hat\\beta_1=3\\)可以解读为，自变量增加一个单位，odds ratio增加\\(exp(\\hat\\beta_1)=20\\)倍。 关心log-odds的变化。log-odds是自变量的线性组合，因此，系数估计还可以解读为，自变量增加一个单位，log-odds增加3个单位。 8.2 题目 这个TASK，我们自学分类问题的建模（可能很多同学在学校没学过这部分知识，自学十分有挑战）。学习材料主要讲述了逻辑回归与决策树的知识，请你仔细阅读。另外，公众号熊大的数据价值回归系列，0-1回归部分，有非常精彩的讲解，请大家认真学习。在学习完这些材料之后，尝试完成作业。具体要求如下。 继续使用TASK 3的消费金融数据。 将原来的因变量【申请人数】做如下处理，如果申请人数是0，保持不变；如果申请人数大于0，处理成1。新生成的这个因变量就是【是否有人申请】。 以是否有人申请为因变量，自己尝试构造有意义的自变量。分别建立逻辑回归模型和决策树模型，理解什么样的产品有人申请。注意，原始数据的很多变量不适合直接做自变量，你需要有理有据地自己构造【有意义的】自变量。 形成一份PDF报告。报告主要由4部分组成： 4.1 一个段落的文字，简单陈述背景和研究问题（注意这个TASK的因变量是0-1类型的）。 4.2 两页左右的篇幅，进行简单的描述分析（注意，因变量是0-1，请选择适合的统计图表进行描述）。无需报告数据说明表，因为没有那么多篇幅了。有逻辑地组织好你的描述分析。 4.3 三页左右的篇幅，报告逻辑回归以及决策树的估计结果、解读、以及预测精度。认真看学习材料，谁敢直接截图代码结果我就直接淘汰你！注意，认真解读结果，我要你有思考的解读！！！再注意，不要出现任何公式，也不要介绍方法和原理，我比你清楚！ 4.4 用几个段落说明，两个方法的结果比较。选择一个角度，说明一下你准备采用哪个方法。注意，时刻记住你的研究目的是什么。 提交一份6页的PDF文件（超页说明你废话多，页数少说明没认真写）。写上报告题目！！！宋体，小四号字体，1.5倍行距。排版整齐。提交之前仔细阅读，避免语病和错别字。我要正式一点的报告，谁再写口语化的、推文风格的报告，我就罚你给狗熊会写一年推文！ DDL：2018年8月2日晚24:00. 7. 别问我写这个行不行，写那个行不行，是不是应该这么写那么写，这不是证明题，没有标准答案！把我当成你的客户、你的老板，站在读者的角度想想什么样的报告会让人眼前一亮！注意，给你3天时间，是让你认真学习仔细打磨报告的。不是让你玩2天拖到最后1天赶一个仓促的报告。 8.3 影响消费贷款有效性的因素分析 8.3.1 背景与研究问题 消费金融作为一种提供消费贷款的现代金融服务方式，正在高速地渗透进人们的日常生活中。数据显示，2017年我国互联网消费金融的交易规模从60亿元猛增到3625亿元，其规模呈现爆发式增长意味着消费金融产品的数量和种类也相应急剧增长。然而，质量并不一定和数量正相关，消费金融产品良莠不齐，有些贷款收到广泛申请客户的钟爱，而有些产品自从发行以来根本没有吸引任何申请客户。为了保证消费金融市场健康、稳定、高效地发展，如何制定出真正有效的、可以吸引申请客户的贷款产品成为各消费金融公司关注的核心问题。 本案例使用了1046家消费金融公司所发放的，贷款金额为10万元、还款期限为12个月的消费贷款明细数据，包括审批时间、担保方式等7个借贷变量，月供、还款总费用等4个还贷变量，以及贷款申请人数变量。其中，因变量为申请人数，当申请人数为0时，代表该贷款没有客户申请，是无效的；当申请人数大于0时，代表实际的申请人数。由此，将因变量处理成0-1变量，即相应贷款是否有人申请，将该变量定义为贷款产品的有效性，通过后续建模挖掘影响贷款产品有效性的重要因素。 8.3.2 描述性分析 借贷变量包括城市、审批时间、放款日期、担保方式、期限最高与最低范围和申请条件。首先将城市变量分为两类：北上广深四个一线城市和其他非一线城市。 由图1可以看出一线城市的贷款产品有效性更高，因此初步判定城市会对消费金融产品的有效性产生影响。由图2可以看出，信用贷的担保方式有效性更高，由于其他三种担保方式的样本量远小于信用贷，可以得出初步结论：担保方式可能对贷款产品有效性有影响，然而，这种影响也可能是由于数据本身的样本量差异形成的。 审批时间和放款日期的总和表示从申请人发出申请至发放贷款的总时长，将二者的和定义为等待时间。由图3则可以看出，有效的贷款产品在申请人发出申请后的平均等待时间（中位数）要明显高于无效的贷款产品，说明客户更青睐具有严格的审批流程与放贷规则的产品，因此，初步认为等待时间对贷款产品有效性有影响。 还贷变量包括月供、还款总费用、月管理费和还款方式。由图4可以看出，有效和无效贷款产品的月供平均水平（中位数）差异并不明显；图5和图6显示，其不同月管理费率和不同总费用的贷款有效性差异也十分微小，初步结论为：月供、月管理费费率和还款总费用可能不影响贷款产品有效性，但是还需要借助后续建模结果进行定量判断。 由图7可以看出，月管理费类型中收取月管理费的产品有效性更高，但是其样本量大于参考月利率收费类型的样本量，并且远大于一次性收费类型的样本量，因此月管理费类型对与贷款的有效性影响也可能是由于数据本身的样本量差异形成的。由图8则可以看出，分期还款的产品有效性最高，但是到期还款和随借随还的产品样本量远小于分期还款的产品，因此，这种影响也可能是由于数据量的差异形成的。 通过对数据的描述性分析，本案例认为贷款产品的借贷变量（包括城市、等待时间和担保方式）和还贷变量（包括月供、还款总费用、月管理费类型、月管理费费率和还款方式）都可能会影响贷款产品的有效与否。为了深入挖掘影响贷款产品有效性的显著因素，本案例将建立影响有效性因素的逻辑回归模型（使用AIC标准选择模型）和决策树模型，并综合模型的复杂程度和预测精度，选择最有利于预测贷款产品有效性的统计模型。 8.3.3 模型建立与选择 8.3.3.1 逻辑回归模型 表1展示了AIC模型的回归结果。 变量 系数估计 p值 备注 截距项 -21.851789 0.033589 NA 一线城市 0.420865 0.002845 基准值：非一线城市 月供 0.002316 0.058691 NA 还款总费用 -1.754429 0.089953 NA 担保方式–抵押贷 1.238128 0.308079 基准值：担保贷 担保方式–信用贷 1.801066 0.131809 NA 担保方式–自由选 0.504714 0.690626 NA 放款日期 -0.120479 0.01399 NA 审批时长 0.321546 &lt;0.001 NA 期限最高范围–18个月 0.221104 0.680911 基准值：12个月 期限最高范围–24个月 0.00529 0.986929 NA 期限最高范围–36个月 0.787114 0.005764 NA 期限最高范围–48个月 1.129317 0.001629 NA 期限最高范围–60个月 0.982672 0.050331 NA 期限最高范围–120个月 1.139409 0.077782 NA 期限最高范围–360个月 -0.773112 0.570084 NA 从估计结果来看，在0.05的显著性水平下，可得出以下结论：若控制其他影响因素不变， （1）对于城市因素而言，一线城市的贷款产品被申请的可能性更大，这说明在北上广深发行的贷款产品更加有效，容易吸引客户； （2）对于放款日期而言，时间越短，产品的有效性越高；对于审批时长而言，审批时间长的更有可能被申请；这说明具有严格的审批流程，同时具有高效的放款流程的贷款更可能收到青睐。 （3）对于还款期限而言，最高范围为4年的贷款最有效，其次是1年的，最难吸引申请客户的是最高期限范围为3年的产品，这说明还款期限的上限给潜在客户带来的心理预期对产品有效性是有影响的。 使用该模型进行预测，选定样本的有效比例62%为阈值，产生的混淆矩阵如表2所示。据此计算出来的错分率、TPR和FPR分别是36.2%、69.2%、45.1%。 ## predict ## real 0 1 ## 0 217 178 ## 1 200 449 8.3.3.2 决策树模型 图9展示了决策树模型的估计结果。从决策树的第一层节点（担保方式）来看，非信用贷类型的产品进入了图的左侧，而左侧的第二层节点告诉我们，在担保贷、抵押贷和自由选三种担保方式下，月供大于9791元的贷款产品是更有可能被人申请（有效的）的产品，这说明当提供要求的抵押物或由第三方提供担保时，每个月还款总额越高，才能越早赎回抵押物，解除第三方的担保风险，因此月供较高的产品更吸引潜在申请人。进一步看决策树的右侧，审批时间超过1.5天，并且月管理费费率小于0.66%的产品是大概率有效的，严格的审批流程和较低的费率当然是申请人较为青睐的。还款的期限范围会对人们的心理预期产生影响，中间的第三个节点表示，期限最高范围在2年之内的贷款是没有人申请的，而最高范围大于2年并且还款总费用小于5700元的产品很可能是有效的；同样地，期限最低范围大于3个月（第六个节点）也会大概率提高产品的有效性。这说明，当还款期限的最高、最低范围都较大时，人们对于自己的还款能力更有把握，从而更可能选择这样的贷款产品。此外，一个有趣的现象是，信用贷产品的月供金额在9016至10000元时更可能被申请，较为明确的月还款范围可能和10万元贷款申请人的平均收入水平有关。 同样地，选定样本的有效比例62%为阈值，用决策树模型预测产生的混淆矩阵如表3所示。据此计算出来的错分率、TPR和FPR分别是29.2%、78.4%、41.8%。 ## predict ## real 0 1 ## 0 230 165 ## 1 140 509 8.3.3.3 模型的比较与选择 根据两个模型的评价指标，决策树模型的错分率和FPR更小，且TPR更大，说明其预测的精度比逻辑回归模型（AIC标准）更高。绘制两个模型的ROC曲线（图10和图11），得到AIC模型的AUC值为0.678，决策树模型的AUC值为0.732，说明决策树模型的预测能力更好。 因此，综合考虑模型的预测精度指标和整体预测能力，选择决策树模型来预测一种贷款产品的有效性（是否有人申请）更佳。 8.4 代码 ###加载需要的程序包### library(data.table) library(vcd) library(pROC) library(rpart) library(rattle) library(magrittr) library(dplyr) rm(list = ls()) #清除工作环境 dat0 &lt;- read.csv(&quot;task_3.csv&quot;) #读入数据，命名dat0 dat1 &lt;- dat0 #备份数据，命名dat1 dat1$apply[which(dat1$apply &gt; 0)] &lt;- 1 #因变量设置为0-1 n &lt;- nrow(dat1) #数据量n ###将管理费类型和费率解析### split_fee &lt;- as.data.frame(tstrsplit(dat1$monthly_fee, &quot; &quot;)) #按照空格将费用类型和费率分别提取出来 colnames(split_fee) &lt;- c(&quot;fee_type&quot;,&quot;fee_rate&quot;) #数据框重命名 dat1 &lt;- cbind(dat1, split_fee) #合并数据框 #转换费率类型 trans &lt;- c() trans &lt;- as.data.frame(matrix(lapply(split_fee[,&quot;fee_rate&quot;], function(x) as.numeric(sub(&quot;%&quot;, &quot;&quot;, x))/100))) split_fee$fee_rate &lt;- as.numeric(trans$V1) #把管理费率从字符型百分数转化成数值型 dat1 &lt;- dat1 %&gt;% select(-monthly_fee) #从原数据框删除解析前的一列 dat1 &lt;- cbind(dat1, split_fee) #合并新的数据框 ###将城市划分为一线和非一线### dat1$city_flag &lt;- &quot;非一线&quot; dat1[city %in% c(&quot;北京&quot;,&quot;上海&quot;,&quot;广州&quot;,&quot;深圳&quot;), city_flag := &quot;一线&quot;] ###棘状图### city_spine &lt;- table(dat1$city_flag, dat1$apply) spine(city_spine, main = &quot;城市因素&quot;, ylab = &quot;是否申请&quot;) fee_type_spine &lt;- table(dat1$fee_type, dat1$apply) spine(fee_type_spine, main = &quot;月管理费类型&quot;, ylab = &quot;是否申请&quot;) insure_way_spine &lt;- table(dat1$way_to_insure, dat1$apply) spine(insure_way_spine, main = &quot;担保方式&quot;, ylab = &quot;是否申请&quot;) pay_way_spine &lt;- table(dat1$way_to_pay, dat1$apply) spine(pay_way_spine, main = &quot;还款方式&quot;, ylab = &quot;是否申请&quot;) dat1[which(dat1$wait_for_money == &quot;审批后当日（审批为3&quot;),&quot;wait_for_money&quot;] &lt;- 0 #将文字改为意思相同的数字 dat1$wait_for_money &lt;- as.numeric(as.character(dat1$wait_for_money)) #转化等待贷款时间的数据类型为数值型 dat1 &lt;- data.table(dat1) #转化为data.table dat1$total_wait_time &lt;- dat1$wait_for_money + dat1$wait_for_permssion #计算拿到贷款前的等待时间总长 #等待时间# boxplot( log(total_wait_time) ~ apply, dat1, col = c(&quot;lightblue&quot;, &quot;lightpink&quot;), names = c(&quot;未申请&quot;, &quot;申请&quot;), ylab = &quot;对数等待时间/天&quot;) #月供# boxplot( monthly_pay ~ apply, dat1, col = c(&quot;lightblue&quot;, &quot;lightpink&quot;), names = c(&quot;未申请&quot;, &quot;申请&quot;), ylab = &quot;月供/元&quot;) #还款总费用# boxplot( total_fee ~ apply, dat1, col = c(&quot;lightblue&quot;, &quot;lightpink&quot;), names = c(&quot;未申请&quot;, &quot;申请&quot;), ylab = &quot;还款总费用/万元&quot;) #月管理费费率# boxplot( fee_rate ~ apply, dat1, col = c(&quot;lightblue&quot;, &quot;lightpink&quot;), names = c(&quot;未申请&quot;, &quot;申请&quot;), ylab = &quot;月管理费费率&quot;) ###建立模型### dat1 &lt;- na.omit(dat1) #去除空值 n1 &lt;- nrow(dat1) temp_data &lt;- dat1[,.(city_flag,monthly_pay,total_fee,way_to_pay,way_to_insure,fee_type,fee_rate,wait_for_money,wait_for_permssion,apply,ddl_low_limit,ddl_up_limit)] temp_data$city_flag &lt;- factor(temp_data$city_flag) #城市、贷款期限上下限转化为因子型 temp_data$ddl_low_limit &lt;- factor(temp_data$ddl_low_limit) temp_data$ddl_up_limit &lt;- factor(temp_data$ddl_up_limit) temp_data$apply &lt;- as.factor(temp_data$apply) #贷款申请人数转化为因子型 #logistic回归模型# glm_full &lt;- glm(apply ~ ., family = binomial(link = &quot;logit&quot;), data = temp_data) #全模型 glm_AIC &lt;- step(glm_full, k = 2, trace = FALSE) #AIC标准筛选变量 #summary(glm_AIC) #查看模型 predict &lt;- predict(glm_AIC,type=&#39;response&#39;, newdata = temp_data) #预测新的数据 #混淆矩阵# real &lt;- temp_data$apply #实际值 ratio_yes &lt;- nrow(temp_data[which(temp_data$apply ==1 ),])/nrow(temp_data) #因变量取值为1的比例 table(real,predict = ifelse(predict &gt; ratio_yes,1,0)) #混淆矩阵表 #ROC曲线# modelroc &lt;- roc(temp_data$apply,predict) #根据预测值和实际值绘制ROC曲线 plot(modelroc, print.auc = TRUE, print.thres = TRUE, xlab = &quot;特异度&quot;, ylab = &quot;灵敏度&quot;) #绘制曲线 #决策树模型# treelm &lt;- rpart(apply ~ ., data = temp_data) #决策树模型 fancyRpartPlot(treelm, sub = &quot;&quot;) #绘制决策树 predict_tree &lt;- predict(treelm,type=&#39;prob&#39;, newdata = temp_data) #预测新的数据 table(real,predict = ifelse(predict_tree[,2] &gt; ratio_yes,1,0)) #混淆矩阵表 treeroc &lt;- roc(temp_data$apply, predict_tree[,2]) #根据预测值和实际值绘制ROC曲线 plot(treeroc, print.auc = TRUE, print.thres = TRUE, xlab = &quot;特异度&quot;, ylab = &quot;灵敏度&quot;) #绘制曲线 "],
["final.html", "第 9 章 Fianl Project 9.1 作品Slides 9.2 代码", " 第 9 章 Fianl Project 9.1 作品Slides 9.2 代码 #环境设置# rm(list = ls()) setwd(&quot;D:/Melody_Ren/onedrive/Rworkspace/bearclub/final&quot;) # dat0 &lt;- read_excel(&quot;final_data.xlsx&quot;) #从网上爬下来的没有去重的数据 #加载需要的程序包# library(data.table) library(magrittr) library(readxl) library(ggplot2) library(lubridate) library(viridis) library(jiebaR) library(wordcloud2) library(reshape2) library(stringr) library(text2vec) library(openxlsx) library(tmcn) #背景描述# #绘图theme函数 my_ggtheme &lt;- function(){ theme(axis.text.x = element_text(size =18)) + theme(axis.text.y = element_text(size =18)) + theme(axis.title.x = element_text(size =20)) + theme(axis.title.y = element_text(size =20)) + theme(legend.text = element_text(size=15)) + theme(legend.title = element_text(size=15)) + theme(panel.grid.major =element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank(),axis.line = element_line(colour = &quot;black&quot;)) } #数据来源于网络 song &lt;- data.frame(type = c(&quot;古典&quot;,&quot;舞曲&quot;,&quot;电音&quot;,&quot;民谣&quot;,&quot;爵士&quot;,&quot;其他&quot;,&quot;摇滚&quot;,&quot;说唱&quot;,&quot;R&amp;B&quot;,&quot;流行&quot;), like = c(0.74,5.67,20.44,14.19,1.26,11.55,7.24,3.05,5.72,30.14)) ggplot(song, aes(x = reorder(type, - like), y = like))+ geom_bar(stat = &quot;identity&quot;, fill = &quot;orange2&quot;) + labs(x = &quot;音乐风格&quot;, y = &quot;用户偏好比例（%）&quot;) + my_ggtheme() # data process dat000 &lt;- read.xlsx(&quot;final_data.xlsx&quot;) # 读取数据 # 处理文本 dat000$play_num = gsub(&#39;播放量：&#39;, &#39;&#39;, dat000$play_num) dat000$collect_num = gsub(&#39;收藏量：&#39;, &#39;&#39;, dat000$collect_num) dat000$comment = gsub(&#39;评论\\\\(([0-9]+)\\\\)&#39;, &#39;\\\\1&#39;, dat000$comment) dat000$album = gsub(&#39;专辑：&#39;, &#39;&#39;, dat000$album) dat000$language = gsub(&#39;语种：&#39;, &#39;&#39;, dat000$language) dat000$genre = gsub(&#39;流派：&#39;, &#39;&#39;, dat000$genre) #从album专辑列选出含有“语种”的数据 wrong_language_index &lt;- grep(&quot;^语种&quot;,dat000$album) dat000[wrong_language_index,&quot;language&quot;] &lt;- dat000[wrong_language_index,&quot;album&quot;] dat000[wrong_language_index,&quot;album&quot;] &lt;- NA #由于数据抓取是按照民谣歌单选择，而非直接对民谣歌手选择，因此数据包含较多流行歌手(I don&#39;t know why)，对一些明显不是民谣曲风的歌手进行手动剔除 wrong_singer &lt;- c(&quot;周笔畅&quot;,&quot;陈小春&quot;,&quot;GAI&quot;,&quot;G.E.M. 邓紫棋&quot;,&quot;杨宗纬&quot;,&quot;苏打绿&quot;,&quot;莫文蔚&quot;,&quot;李荣浩&quot;,&quot;杨坤/郭采洁&quot;,&quot;薛之谦&quot;,&quot;陈奕迅&quot;,&quot;许嵩&quot;,&quot;五月天&quot;, &quot;容祖儿&quot;, &quot;王源&quot;,&quot;孙燕姿&quot;,&quot;陈小春&quot;,&quot;梁静茹&quot;,&quot;张惠妹&quot;,&quot;汪苏泷&quot;,&quot;徐良/汪苏泷&quot;,&quot;林俊杰&quot;,&quot;周杰伦&quot;,&quot;范玮琪&quot;,&quot;张杰&quot;,&quot;范晓萱&quot;,&quot;蔡健雅&quot;,&quot;张杰&quot;,&quot;周笔畅&quot;,&quot;华晨宇&quot;) dat000 = dat000[!dat000$singer %in% wrong_singer, ] # 删了100+行 # 处理时间 launch_time_df = data.frame(tstrsplit(dat000$launch_time, &#39;：&#39;)) colnames(launch_time_df) = c(&quot;time_type&quot;,&quot;time&quot;) dat000 = cbind(dat000, launch_time_df) dat000$launch_time = NULL # 删除原来不需要的列 # 删除无评论的数据 dat000$comment = as.numeric(dat000$comment) dat000 = dat000[!is.na(dat000$comment), ] # 删了347 # colnames(dat000) &lt;- c(&#39;歌单名&#39;, &#39;播放量&#39;, &#39;收藏量&#39;, &#39;评论数&#39;, &#39;播放列表链接&#39;, # &#39;歌曲名&#39;, &#39;歌手&#39;, &#39;专辑&#39;, &#39;语种&#39;, &#39;流派&#39;, &#39;歌词&#39;, &#39;歌曲链接&#39;, # &#39;发行时间类型&#39;, &#39;发行时间&#39;) #选出语种符合要求的数据 want_language &lt;- c(&quot;国语&quot;,&quot; 纯音乐&quot;,&quot; 粤语&quot;,&quot; 其他&quot;,&quot; 潮汕语&quot;,&quot; 台语&quot; ) #由于限定华语民谣，选出确实为华语的歌曲 dat000$language &lt;- as.character(dat000$language) dat000 &lt;- dat000[which(dat000$language %in% want_language),] # 剩下3866行 write.csv(dat000, file=&quot;song.csv&quot;, row.names=FALSE) # 经过清洗的数据，储存为song.csv # 提取歌曲id library(readr)# 读入数据 dat0 &lt;- read.csv(&quot;song.csv&quot;) # colnames(dat0) &lt;- c(&quot;list_name&quot;, &quot;play_num&quot;, &quot;collect_num&quot;, # &quot;comment&quot;, &quot;list_url&quot;, &quot;song_name&quot; , &quot;singer&quot;, # &quot;album&quot;, &quot;language&quot;, &quot;genre&quot;, &quot;lyrics&quot;, # &quot;song_url&quot;, &quot;time_type&quot;, &quot;time&quot;) # 抽取歌曲 id dat0$song_id = gsub(&#39;https://y.qq.com/n/yqq/song/&#39;, &#39;&#39;, dat0$song_url, fixed = TRUE) dat0$song_id = gsub(&#39;.html&#39;, &#39;&#39;, dat0$song_id, fixed = TRUE) # 按歌曲 id 去重 song_index &lt;- duplicated(dat0$song_id) song_dat &lt;- dat0[!song_index, ] cat(sprintf(&#39;去重前数据有 %s 行，去重后剩余 %s 行。\\n&#39;, nrow(dat0), nrow(song_dat))) # 描述分析 # 因变量分布图 # 存在几首英文歌词的歌曲，视为脏数据剔除掉 # song_dat &lt;- song_dat[- which(song_dat$language == &quot;英语&quot;),] # song_dat &lt;- song_dat[- which(song_dat$song_name == &quot;离家五百里&quot;),] ggplot(song_dat, aes(x = comment)) + geom_histogram(fill = &quot;orange2&quot;) + # 直方图 scale_x_log10( # log变换 breaks=c(0,50,1e3,1e5), labels=c(&quot;0&quot;,&quot;50&quot;, &quot;1千&quot;, &quot;10万&quot;) ) + # 对热度进行对数变换 xlab(&quot;评论数（对数变换）&quot;) + ylab(&quot;频数&quot;) + # x轴y轴的label my_ggtheme() # 流派和热度的关系 song_dat[which(song_dat$genre == &quot;Alternative&quot;),&quot;chinese&quot;] &lt;- &quot;其他&quot; song_dat[which(song_dat$genre == &quot;Folk&quot;),&quot;chinese&quot;] &lt;- &quot;民谣&quot; song_dat[which(song_dat$genre == &quot;Rock&quot;),&quot;chinese&quot;] &lt;- &quot;摇滚&quot; song_dat[which(song_dat$genre == &quot;Pop&quot;),&quot;chinese&quot;] &lt;- &quot;流行&quot; song_dat[which(song_dat$genre == &quot;Soundtrack&quot;),&quot;chinese&quot;] &lt;- &quot;原声音乐&quot; song_dt &lt;- song_dat[complete.cases(song_dat[,&quot;chinese&quot;]),] #删除流派的空值 song_dt &lt;- data.table(song_dt) song_dt[,chinese := as.character(chinese)] song_dt[, genre_med := median(as.numeric(comment)), by = chinese] ggplot(song_dt, aes(x = reorder(chinese, genre_med), y = log(comment))) + geom_boxplot(varwidth=TRUE, fill = &quot;orange2&quot;, color = &quot;chocolate4&quot;) + labs(x = &quot;&quot;, y = &quot;对数评论数&quot;) + my_ggtheme() # 发行年份和热度的关系 song_dat$time &lt;- ymd(song_dat$time) song_dat1 &lt;- song_dat[which(song_dat$time_type == &#39;发行时间&#39;),] #只看发行时间，不看上传时间 song_dat1 &lt;- song_dat1[complete.cases(song_dat1[,&quot;time&quot;]),] #去除发行年份为空值的 split_year_flag &lt;- function(x){ #设置发行年份flag函数 if ((x &gt; 1976) &amp; (x &lt; 1990)) return(&#39;1990年以前&#39;) if (x &lt; 1995) return(&#39;1990-1995年&#39;) if (x &lt; 2000) return(&#39;1995-2000年&#39;) if (x &lt; 2005) return(&#39;2000-2005年&#39;) if (x &lt; 2010) return(&#39;2005-2010年&#39;) if (x &lt; 2015) return(&#39;2010-2015年&#39;) return(&#39;2015-2018年&#39;) } song_dat1$year_flag = sapply(year(song_dat1$time), split_year_flag) song_dat1 &lt;- data.table(song_dat1) song_dat1[, median_flag := median(as.numeric(comment)), by = &quot;year_flag&quot;] #给每个发行年份分组添加中位数标签 ggplot(song_dat1, aes(x = reorder(year_flag, median_flag), y = log(comment)))+ geom_boxplot(varwidth=TRUE,fill = &quot;orange2&quot;, color = &quot;chocolate4&quot;)+ labs(x = &quot;&quot;,y = &quot;对数评论数&quot;) + my_ggtheme() + theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) #民谣歌手热度TOP15# # song_dat1 &lt;- song_dat1[!song_dat1$singer %in% wrong_singer, ] #删除错误歌手之后的数据 #这几个歌手是什么鬼啊 = = song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;大壮&quot;)),] song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;GALA&quot;)),] song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;刘若英&quot;)),] song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;于文文&quot;)),] song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;买辣椒也用券&quot;)),] song_dat1 &lt;- song_dat1[c(-which(song_dat1$singer == &quot;JC陈泳彤&quot;)),] top_singer &lt;- aggregate(comment~singer,data=song_dat1,sum) %&gt;% as.data.frame() #每个歌手的歌曲总热度 top_singer_10 &lt;- top_singer[order(top_singer$comment, decreasing = TRUE),][1:10,] #按照歌曲总热度从大到小排序 #绘制不同歌手歌曲总热度条形图 ggplot(top_singer_10, aes(x = reorder(singer, - comment), y = comment/10000)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;orange2&quot;) + labs(x = &quot;歌手&quot;, y = &quot;总评论数量/万&quot;) + my_ggtheme() # 文本分析 # 去除作词作曲那些和最后的[展开]# song_dat$lyrics &lt;- as.character(song_dat$lyrics) for (i in 1:nrow(song_dat)){ a &lt;- strsplit(song_dat$lyrics[i], &#39;\\n&#39;) delet_index &lt;- grep(&quot;-|：|展开&quot;,a[[1]]) a[[1]] &lt;- a[[1]][-delet_index] song_dat$lyrics_no_singer[i] &lt;- a } engine_stop &lt;- worker(stop_word = &quot;stopword.txt&quot;) #停用词.txt文档 top100 &lt;- song_dat[order(-song_dat$comment),]$lyrics_no_singer[1:100] #选择热度最高的100首歌的歌词 seg &lt;- segment(unlist(top100),engine_stop) # seg &lt;- segment(top100,engine_stop) #进行分词，并且除去停用词 seg &lt;- seg[nchar(seg)&gt;1] #去除字符长度小于2的词语 seg &lt;- table(seg) #统计词频 seg &lt;- seg[!grepl(&#39;[0-9]+&#39;,names(seg))] #去除数字 seg &lt;- seg[!grepl(&#39;[a-z]+&#39;,names(seg))] #去除字母 # length(seg) #查看处理完后剩余的词数 seg_100 &lt;- sort(seg, decreasing = TRUE)[1:100] #降序排序，并提取出现次数最多的前100个词语 # seg_100 #查看100个词频最高的 # 绘制高频词词云 #设置词云颜色 mycolor &lt;- c(&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;, &#39;#EDB999&#39;, &#39;#E05263&#39;, &quot;#ff7f50&quot;, &quot;#ff8b61&quot;, &quot;#ff9872&quot;,&quot;#ffa584&quot;, &quot;#ffb296&quot;, &quot;#ffbfa7&quot;, &quot;#ffcbb9&quot;, &quot;#ffd8ca&quot;, &quot;#ffe5dc&quot;, &quot;#fff2ed&quot;,&quot;99ffff&quot;,&quot;99cccc&quot;,&quot;66cccc&quot;,&quot;33cccc&quot;,&quot;009999&quot;,&quot;999999&quot;,&quot;ff9999&quot;,&quot;ff9933&quot;,&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;, &#39;#EDB999&#39;, &#39;#E05263&#39;, &quot;#ff7f50&quot;, &quot;#ff8b61&quot;, &quot;#ff9872&quot;, &quot;#ffa584&quot;, &quot;#ffb296&quot;, &quot;#ffbfa7&quot;,&quot;#ffcbb9&quot;, &quot;#ffd8ca&quot;, &quot;#ffe5dc&quot;, &quot;#fff2ed&quot;,&quot;99ffff&quot;,&quot;99cccc&quot;,&quot;66cccc&quot;,&quot;33cccc&quot;,&quot;009999&quot;,&quot;999999&quot;,&quot;ffff99&quot;,&quot;ccccff&quot;,&quot;ffcc66&quot;,&quot;ffcc99&quot;,&quot;ff6666&quot;,&quot;99cc99&quot;,&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;, &#39;#EDB999&#39;, &#39;#E05263&#39;, &quot;#ff7f50&quot;, &quot;#ff8b61&quot;, &quot;#ff9872&quot;,&quot;#ffa584&quot;, &quot;#ffb296&quot;, &quot;#ffbfa7&quot;, &quot;#fff2ed&quot;,&quot;99ffff&quot;,&quot;99cccc&quot;,&quot;66cccc&quot;,&quot;33cccc&quot;,&quot;009999&quot;,&quot;999999&quot;,&quot;ff9999&quot;,&quot;ff9933&quot;,&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;, &#39;#EDB999&#39;, &#39;#E05263&#39;, &quot;#ff7f50&quot;, &quot;#ff8b61&quot;, &quot;#ff9872&quot;, &quot;#ffa584&quot;, &quot;#ffb296&quot;,&quot;#ff8b61&quot;, &quot;#ff9872&quot;, &quot;#ffa584&quot;, &quot;#ffb296&quot;, &quot;#ffbfa7&quot;,&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;,&quot;#fff2ed&quot;,&quot;99ffff&quot;,&quot;99cccc&quot;,&quot;66cccc&quot;,&quot;33cccc&quot;,&quot;009999&quot;,&quot;999999&quot;,&quot;ff9999&quot;,&quot;ff9933&quot;,&#39;#69A2B0&#39;, &#39;#659157&#39;, &#39;#A1C084&#39;, &#39;#EDB999&#39;, &#39;#E05263&#39;, &quot;#ff7f50&quot;) wordcloud2(seg_100, size = 0.8, color = mycolor, fontFamily = &quot;微软雅黑&quot;) #绘制词云 # 押韵程度和最多押韵韵脚 song_dat2 &lt;- song_dat[which(song_dat$lyrics_no_singer != &quot;&quot;),] #去除歌词为空值的 # song_dat2 &lt;- song_dat2[nchar(song_dat2$lyrics_no_singer) &gt; 20,] #选择出歌词字符数量大于20的 yunmu &lt;- c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;,&quot;v&quot;) #几首歌词有bug的歌曲 song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;0035Qwhy3M1jDE&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;003GDcQk0q8DNt&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;001wURcl1L4Mop&quot;),] for (i in 931:nrow(song_dat2)){ no_na_lyrics &lt;- song_dat2$lyrics_no_singer[i][[1]] #提出每首歌歌词(格式是list) #一首歌每句歌词的最后一个完整拼音 last_pinyin &lt;- vector(length = length(no_na_lyrics)) lastword &lt;- vector(length = length(no_na_lyrics)) for (m in 1:length(last_pinyin)){ lastword[m] &lt;- substring(no_na_lyrics[m], str_length(no_na_lyrics[m]), str_length(no_na_lyrics[m])) lastword &lt;- lastword[!grepl(&#39;[0-9]+&#39;,lastword)] lastword &lt;- lastword[!grepl(&#39;[a-z]+&#39;,lastword)] lastword &lt;- lastword[!grepl(&#39;[A-Z]+&#39;,lastword)] last_pinyin[m] &lt;- toPinyin(lastword[m]) } last_pinyin &lt;- last_pinyin[!grepl(&#39;NA&#39;,last_pinyin)] #一首歌每句歌词最后一个字的韵母 last_yunmu &lt;- vector(length = length(last_pinyin)) for (j in 1:length(last_pinyin)){ for(k in 1:str_length(last_pinyin[j])){ #else表示到韵母了 if(!substring(last_pinyin[j], k, k) %in% yunmu){ temp &lt;- substring(last_pinyin[j], k+1, str_length(last_pinyin[j])) } else last_yunmu[j] &lt;- temp } } last_yunmu &lt;- last_yunmu[!grepl(&#39;NA&#39;,last_yunmu)] song_dat2$yayun_scale[i] &lt;- table(last_yunmu) %&gt;% max() song_dat2$yunjiao[i] &lt;- rownames(table(last_yunmu))[which.max(table(last_yunmu))] } yunjiao_table &lt;- table(song_dat2$yunjiao) yunjiao_frame &lt;- sort(yunjiao_table, decreasing = TRUE)[1:10] %&gt;% as.data.frame() #热门韵脚 ggplot(yunjiao_frame, aes(x = Var1, y = Freq)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;orange2&quot;) + labs(x = &quot;韵脚&quot;, y = &quot;频数&quot;) + my_ggtheme() ggplot(song_dat2, aes(x = yayun_scale, y = comment/10000)) + geom_point(size = 4, color = &quot;orange2&quot;, alpha = 0.7) + geom_vline(aes(xintercept = 24), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + geom_vline(aes(xintercept = 15), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + labs(x = &quot;押韵程度&quot;, y = &quot;评论数/万&quot;) + my_ggtheme() yunjiao_box &lt;- song_dat2[which(song_dat2$yunjiao %in% yunjiao_frame$Var1),] for (i in 1:nrow(yunjiao_box)) { yunjiao_box$median_yunjiao[i] &lt;- median(yunjiao_box$comment[which(yunjiao_box$yunjiao == yunjiao_box$yunjiao[i])]) } ggplot(yunjiao_box,aes(x = reorder(yunjiao, median_yunjiao), y = log(comment))) + geom_boxplot(varwidth=TRUE,fill = &quot;orange2&quot;, color = &quot;chocolate4&quot;) + labs(x = &quot;韵脚&quot;, y = &quot;对数评论数&quot;) + my_ggtheme() # 句子长度，重复数量，句子数量 # 有bug的歌曲 song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;002aEfK42ZJne3&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;002nEJxP4SzmVr&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;000gyfCe3og7HL&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;000M3yim4cbxkc&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;003wCfWs3RT5pc&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;002SAzyy1aY8jK&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;000qoKip1AYapG&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;003W3mdN1hncrS&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;000mE2ff3uKO6L&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;001mCAni3WN043&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;002axyOV0Lvpbb&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;001Oaacc48zoBn&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;004eqh310KvtLZ&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;0045J9sT2tCkCV&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;000ppUwW1sb2lS&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;001H5tpN3WRmZd&quot;),] song_dat2 &lt;- song_dat2[-which(song_dat2$song_id == &quot;002ASPoZ3cVea3&quot;),] for(j in 1:nrow(song_dat2)){ count_sentence &lt;- 0 # b &lt;- onlysong_no_outlier$lyrics_no_singer[j] #取出每一行的歌词 # b &lt;- gsub(pattern=&quot;\\r\\n\\r\\n&quot;, replacement=&quot; &quot;, b,fixed=T) %&gt;% # gsub(pattern=&quot;\\r\\n&quot;, replacement=&quot; &quot;, b,fixed=T) %&gt;% # gsub(pattern=&quot;\\t&quot;, replacement=&quot;&quot;, b,fixed=T) #去掉所有的空格换行 # no_na_lyrics &lt;- strsplit(b, &#39; &#39;)[[1]][-which(strsplit(b, &#39; &#39;)[[1]] == &quot;&quot;)] no_na_lyrics &lt;- song_dat2$lyrics_no_singer[j][[1]] #提出每首歌歌词(格式是list) no_na_lyrics &lt;- no_na_lyrics[!grepl(&#39;[a-z]+&#39;,no_na_lyrics)] #去除字母 song_dat2$sentence_num[j] &lt;- length(no_na_lyrics) #句子数量 song_dat2$sentence_length[j] &lt;- str_length(no_na_lyrics) %&gt;% mean()#每首歌平均句长 for (i in (1:(length(no_na_lyrics)-1))){ if (no_na_lyrics[i] == no_na_lyrics[i+1]){ count_sentence &lt;- count_sentence + 1 } } song_dat2$repeat_lyrics[j] &lt;- count_sentence } # 句子数量（篇幅） ggplot(song_dat2, aes(x = sentence_num, y = comment)) + geom_point(size = 4, color = &quot;orange2&quot;, alpha = 0.7) + labs(x = &quot;句子数量&quot;, y = &quot;评论数&quot;) + geom_vline(aes(xintercept = 25), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + geom_vline(aes(xintercept = 55), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + my_ggtheme() # 平均句长 ggplot(song_dat2, aes(x = sentence_length, y = comment)) + geom_point(size = 4, color = &quot;orange2&quot;, alpha = 0.7) + labs(x = &quot;平均句子长度/字&quot;, y = &quot;评论数&quot;) + geom_vline(aes(xintercept = 6), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + geom_vline(aes(xintercept = 9), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + my_ggtheme() # 重复手法 ggplot(song_dat2,aes(x = repeat_lyrics, y = comment)) + geom_point(color = &quot;orange2&quot;, size = 4, alpha = 0.7) + #geom_bar(stat = &quot;identity&quot;,fill = &quot;orange2&quot;, width = 1) + labs(x = &quot;歌词重复数&quot;, y = &quot;评论数&quot;) + my_ggtheme() # 工整句式 for(j in 1:nrow(song_dat2)){ # count_sentence &lt;- 0 b &lt;- onlysong_no_outlier$lyrics_no_singer[j] #取出每一行的歌词 b &lt;- gsub(pattern=&quot;\\r\\n\\r\\n&quot;, replacement=&quot; &quot;, b,fixed=T) %&gt;% gsub(pattern=&quot;\\r\\n&quot;, replacement=&quot; &quot;, b,fixed=T) %&gt;% gsub(pattern=&quot;\\t&quot;, replacement=&quot;&quot;, b,fixed=T) #去掉所有的空格换行 no_na_lyrics &lt;- strsplit(b, &#39; &#39;)[[1]][-which(strsplit(b, &#39; &#39;)[[1]] == &quot;&quot;)] #分成一句一句的 no_na_lyrics &lt;- no_na_lyrics[!grepl(&#39;[a-z]+&#39;,no_na_lyrics)] #去除字母 # onlysong_no_outlier$sentence_num[j] &lt;- length(no_na_lyrics) #句子数量 # onlysong_no_outlier$sentence_length[j] &lt;- str_length(no_na_lyrics) %&gt;% mean()#每首歌平均句长 onlysong_no_outlier$sentence_tidy[j] &lt;- str_length(no_na_lyrics) %&gt;% table() %&gt;% max() #句子长度一样的最多数量 } ggplot(onlysong_no_outlier, aes(x = sentence_tidy, y = comment)) + geom_point(size = 4, color = &quot;orange2&quot;, alpha = 0.7) + labs(x = &quot;句子工整程度&quot;, y = &quot;评论数&quot;) + geom_vline(aes(xintercept = 10), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + geom_vline(aes(xintercept = 17), colour=&quot;#990000&quot;, linetype=&quot;dashed&quot;, size = 1.5) + my_ggtheme() # 词向量 + 聚类 # 把分好的词写进.txt文档 write.table(seg, &quot;lyrics.txt&quot;) library(tmcn) # 加载训练词向量程序包 # 自编译word2vec函数(参考链接：https://gist.github.com/badbye/a41db116e6f3490d0505aba704661050) word2vec &lt;- function (train_file, output_file, binary=1, # output format, 1-binary, 0-txt cbow=0, # skip-gram (0) or continuous bag of words (1) num_threads = 1, # num of workers num_features = 300, # word vector dimensionality window = 10, # context / window size min_count = 40, # minimum word count sample = 1e-3, # downsampling of frequent words classes = 0 ){ if (!file.exists(train_file)) stop(&quot;Can&#39;t find the trsin file!&quot;) train_dir &lt;- dirname(train_file) if (missing(output_file)) { output_file &lt;- gsub(gsub(&quot;^.*\\\\.&quot;, &quot;&quot;, basename(train_file)), &quot;bin&quot;, basename(train_file)) output_file &lt;- file.path(train_dir, output_file) } outfile_dir &lt;- dirname(output_file) if (!file.exists(outfile_dir)) dir.create(outfile_dir, recursive = TRUE) train_file &lt;- normalizePath(train_file, winslash = &quot;/&quot;, mustWork = FALSE) output_file &lt;- normalizePath(output_file, winslash = &quot;/&quot;, mustWork = FALSE) OUT &lt;- .C(&quot;CWrapper_word2vec&quot;, train_file = as.character(train_file), output_file = as.character(output_file), binary = as.character(binary), num_threads=as.character(num_threads), num_features=as.character(num_features), window = as.character(window), min_count=as.character(min_count), sample=sample,classes=as.character(classes) ) class(OUT) &lt;- &quot;word2vec&quot; names(OUT)[2] &lt;- &quot;model_file&quot; cat(paste(&quot;The model was generated in &#39;&quot;, dirname(output_file), &quot;&#39;!\\n&quot;, sep = &quot;&quot;)) return(OUT) } word2vec(&quot;lyrics.txt&quot;, &quot;vector.txt&quot;) # 将分好的词转换为词向量，储存文件在本地 vector_word &lt;- read.table(&quot;vector.txt&quot;) # 读入训练好的词向量 seg_50 &lt;- sort(seg, decreasing = TRUE)[1:50] # 提取出现次数最多的前50个词语，用来做聚类分析 vec_top_100 &lt;- vector_word[which(vector_word$V1 %in% names(seg_50)),] # 选出向量里对应的最热门的50词 vec_top_100_1 &lt;- vec_top_100[,-1] # 去掉标题（也许吧） vec_cluster_top &lt;- scale(vec_top_100_1) # 标准化 library(factoextra) # 加载需要的程序包 #设置随机数种子 set.seed(1000) #确定最佳聚类个数，使用组内平方误差和法 fviz_nbclust(vec_cluster_top, kmeans, method=&quot;wss&quot;) + geom_vline(xintercept=4,linetype=2) #根据最佳聚类个数，进行kmeans聚类 res_top &lt;- kmeans(vec_cluster_top,4) #将分类结果放入原数据集中 recombine_vec_top &lt;- cbind(vec_top_100,res_top$cluster) #查看最终聚类图形 fviz_cluster(res_top, data = vec_top_100_1) + labs(fill = &quot;类别&quot;, color = &quot;类别&quot;, shape =&quot;类别&quot;) + my_ggtheme() # 分别储存四类词语 cluster_1 &lt;- as.character(recombine_vec_top[which(recombine_vec_top$`res_top$cluster` == 1),&quot;V1&quot;]) cluster_2 &lt;- as.character(recombine_vec_top[which(recombine_vec_top$`res_top$cluster` == 2),&quot;V1&quot;]) cluster_3 &lt;- as.character(recombine_vec_top[which(recombine_vec_top$`res_top$cluster` == 3),&quot;V1&quot;]) cluster_4 &lt;- as.character(recombine_vec_top[which(recombine_vec_top$`res_top$cluster` == 4),&quot;V1&quot;]) # 词性分析 # top100热门歌词性（101:N歌词同理） engine_stop &lt;- worker(type = &quot;tag&quot;,stop_word = &quot;D:/data/stopword.txt&quot;)# 重新设置type，结果输出席次词性 seg_manual_stop_100 &lt;- segment(seg_100,engine_stop) # 分词 seg_manual_stop_100 &lt;- seg_manual_stop_100[nchar(seg_manual_stop_100)&gt;1] # 去掉长度小于2的词语 word_type_top &lt;- seg_manual_stop_100 # 备份 # seg_manual_stop_100 &lt;- word_type_top # 取备份 qseg_top_frame &lt;- as.matrix(seg_manual_stop_100) # 转换成矩阵储存 word_type_top_frame &lt;- data.frame(word = qseg_top_frame[,1], type = rownames(qseg_top_frame)) # 转换成数据框 word_type_top_frame$word &lt;- as.character(word_type_top_frame$word) # 数据形式转换 word_type_top_frame$type &lt;- as.character(word_type_top_frame$type) # 数据形式转化 table(word_type_top_frame$type) # 统计不同词性的比例 word_type_top_frame &lt;- word_type_top_frame[c(- which(word_type_top_frame$type == &#39;eng&#39;)),] # 删除英文词 # 根据词性对应表，把英文改成中文（写得很麻烦，需要简化），参考： # split_year_flag &lt;- function(x){ #设置发行年份flag函数 # if ((x &gt; 1976) &amp; (x &lt; 1990)) return(&#39;1990年以前&#39;) # if (x &lt; 1995) return(&#39;1990-1995年&#39;) # if (x &lt; 2000) return(&#39;1995-2000年&#39;) # if (x &lt; 2005) return(&#39;2000-2005年&#39;) # if (x &lt; 2010) return(&#39;2005-2010年&#39;) # if (x &lt; 2015) return(&#39;2010-2015年&#39;) # return(&#39;2015-2018年&#39;) # } word_type_top_frame[which(word_type_top_frame$type == &quot;a&quot;),&quot;chi_type&quot;] &lt;- &quot;形容词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;ad&quot;),&quot;chi_type&quot;] &lt;- &quot;副形词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;an&quot;),&quot;chi_type&quot;] &lt;- &quot;名形词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;b&quot;),&quot;chi_type&quot;] &lt;- &quot;区别词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;c&quot;),&quot;chi_type&quot;] &lt;- &quot;连词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;d&quot;),&quot;chi_type&quot;] &lt;- &quot;副词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;e&quot;),&quot;chi_type&quot;] &lt;- &quot;叹词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;f&quot;),&quot;chi_type&quot;] &lt;- &quot;方位词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;i&quot;),&quot;chi_type&quot;] &lt;- &quot;成语&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;j&quot;),&quot;chi_type&quot;] &lt;- &quot;简称略语&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;l&quot;),&quot;chi_type&quot;] &lt;- &quot;习用语&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;m&quot;),&quot;chi_type&quot;] &lt;- &quot;数词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;mq&quot;),&quot;chi_type&quot;] &lt;- &quot;数量词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;n&quot;),&quot;chi_type&quot;] &lt;- &quot;名词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;nr&quot;),&quot;chi_type&quot;] &lt;- &quot;人名&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;nrfg&quot;),&quot;chi_type&quot;] &lt;- &quot;未知&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;nrt&quot;),&quot;chi_type&quot;] &lt;- &quot;未知&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;ns&quot;),&quot;chi_type&quot;] &lt;- &quot;地名&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;nt&quot;),&quot;chi_type&quot;] &lt;- &quot;机构团体&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;nz&quot;),&quot;chi_type&quot;] &lt;- &quot;其他专名&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;o&quot;),&quot;chi_type&quot;] &lt;- &quot;拟声词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;p&quot;),&quot;chi_type&quot;] &lt;- &quot;介词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;q&quot;),&quot;chi_type&quot;] &lt;- &quot;量词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;r&quot;),&quot;chi_type&quot;] &lt;- &quot;代词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;s&quot;),&quot;chi_type&quot;] &lt;- &quot;处所词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;t&quot;),&quot;chi_type&quot;] &lt;- &quot;时间词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;v&quot;),&quot;chi_type&quot;] &lt;- &quot;动词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;vn&quot;),&quot;chi_type&quot;] &lt;- &quot;名动词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;vq&quot;),&quot;chi_type&quot;] &lt;- &quot;未知&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;x&quot;),&quot;chi_type&quot;] &lt;- &quot;非语素字&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;y&quot;),&quot;chi_type&quot;] &lt;- &quot;语气词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;z&quot;),&quot;chi_type&quot;] &lt;- &quot;状态词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;rr&quot;),&quot;chi_type&quot;] &lt;- &quot;人称代词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;rz&quot;),&quot;chi_type&quot;] &lt;- &quot;指示代词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;u&quot;),&quot;chi_type&quot;] &lt;- &quot;助词&quot; word_type_top_frame[which(word_type_top_frame$type == &quot;vd&quot;),&quot;chi_type&quot;] &lt;- &quot;副动词&quot; word_type_top_frame &lt;- table(word_type_top_frame$chi_type) # 统计不同词性的频数 word_type_top_10 &lt;- sort(word_type_top_frame, decreasing = TRUE)[1:10] # 最热门的10种词性 top_10_type_top &lt;- as.data.frame(word_type_top_10) # 把table转换成数据框 top_10_type_top$Var1 &lt;- as.character(top_10_type_top$Var1) # 改变数据形式 top_10_type_top &lt;- top_10_type_top[c( -which(top_10_type_top$Var1 == &#39;非语素字&#39;)),] # 删除一种少见的词性 top_10_type_top$por &lt;- top_10_type_top$Freq / sum(top_10_type_top$Freq) # 统计各词性所占比例 # 绘制词性偏好条形图 top_word_type &lt;- top_10_type_top ggplot(top_word_type,aes(x = reorder(Var1,-por), y = por)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;orange2&quot;) + labs(x = &quot;词性&quot;,y = &quot;占比&quot;) + my_ggtheme() # 对比top100和101:N的各词性比例 library(reshape2) # 数据框整形程序包 total_word_type &lt;- merge(top_word_type, bottom_word_type, &quot;Var1&quot;) # 合并两个数据框 total_word_type &lt;- total_word_type[,c(&quot;Var1&quot;,&quot;por.x&quot;,&quot;por.y&quot;)] # 取出不重复的列 total_word_type_melt &lt;- melt(total_word_type, var.id = &quot;Var1&quot;) # 变成长数据框 # 绘制两种情况的词性占比条形图（对比图） ggplot(total_word_type_melt,aes(x = reorder(Var1,-value), y = value, fill = variable)) + geom_bar(position=&quot;dodge&quot;,stat = &quot;identity&quot;) + labs(x = &quot;词性&quot;,y = &quot;占比&quot;, fill = &quot;歌曲类别&quot;) + my_ggtheme() + scale_fill_manual(labels = c(&quot;TOP100&quot;,&quot;其他&quot;), values = c(&#39;orange2&#39;, &#39;chocolate4&#39;)) "],
["sound.html", "A 附录", " A 附录 附录里面还啥都没有呢！因为没有什么其他文件需要放进来。 "],
["references.html", "参考文献", " 参考文献 "]
]
